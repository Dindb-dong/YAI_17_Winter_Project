# %pip install transformers opencv-python pillow numpy google-genai python-dotenv matplotlib
# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# %pip install triton
# %pip install decord
# print("í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ ëë‚¬ìŠµë‹ˆë‹¤.")

import os
import torch  # type: ignore
import torchvision.io as io  # type: ignore
import torchvision.transforms.functional as F  # type: ignore
import json
import datetime
import numpy as np  # type: ignore
from PIL import Image  # type: ignore
from transformers import CLIPProcessor, CLIPModel, Blip2Processor, Blip2ForConditionalGeneration  # type: ignore
from typing import List 
from google import genai  # type: ignore
import json
import re
from dotenv import load_dotenv  # type: ignore
import time
import random
import matplotlib.pyplot as plt  # type: ignore
from matplotlib.patches import Circle  # type: ignore
import shutil

# ## 3. Gemini API ì„¤ì • & Model Manager
# 
# Gemini API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³ , CLIP ë° BLIP-2 ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.


# ==========================================
# Gemini API ì„¤ì • (AdaptiveSearchEngine ë‚´ë¶€ í˜¹ì€ ì™¸ë¶€ì— ì„ ì–¸)
# ==========================================
# .env íŒŒì¼ ë¡œë“œ
# load_dotenv()

# API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ì½”ë© í™˜ê²½ì—ì„œëŠ” ë¡œë”©ì´ ë‹¤ë¦„
# ì¢Œì¸¡ íŒ¨ë„ ì—´ì‡  ëª¨ì–‘ í´ë¦­ -> ìƒˆ ë³´ì•ˆ ë¹„ë°€ ì¶”ê°€ -> ì´ë¦„: GEMINI_API_KEY, ê°’: ì‹¤ì œ API í‚¤ê°’ ë”°ì˜´í‘œ ì—†ì´ ê·¸ëŒ€ë¡œ.
from google.colab import userdata  # type: ignore
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
client = genai.Client(api_key=GEMINI_API_KEY, http_options=genai.types.HttpOptions(api_version="v1"))
if client is not None:
    print("Gemini Client initialized successfully")
else:
    print("Gemini Client initialization failed")
    exit()
# # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
# for model in client.models.list():
#     print(f"Model Name: {model.name}, Supported Methods: {model.supported_actions}")
# exit()

# ==========================================
# 1. Model Manager (CLIP & BLIP-2)
# ==========================================
class ModelManager:
    def __init__(self, use_blip=False, device=None):
        start_time = time.time()
        print("Initializing ModelManager...")
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading models on {self.device}...")

        # Load CLIP (Base Model)
        clip_start = time.time()
        self.clip_processor = CLIPProcessor.from_pretrained(
            "openai/clip-vit-base-patch32",
            use_fast=True  # ì´ ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ Rust ê¸°ë°˜ì˜ ë¹ ë¥¸ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        )
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
        self.clip_model.eval()
        clip_time = time.time() - clip_start
        print(f"CLIP Model loaded ({clip_time:.2f}ì´ˆ)")

        # Load BLIP-2 (Refinement Model) - Optional
        self.use_blip = use_blip
        self.blip_processor = None
        self.blip_model = None
        blip_time = 0.0

        if self.use_blip:
            print("Loading BLIP-2 (this might take memory)...")
            blip_start = time.time()
            self.blip_processor = Blip2Processor.from_pretrained(
                "Salesforce/blip2-opt-2.7b",
                use_fast=True)
            self.blip_model = Blip2ForConditionalGeneration.from_pretrained(
                "Salesforce/blip2-opt-2.7b", dtype=torch.float16, 
                device_map={"": self.device} # í˜„ì¬ ì§€ì •ëœ GPU ì¥ì¹˜ì—ë§Œ í• ë‹¹
            ).to(self.device)
            self.blip_model.eval()
            blip_time = time.time() - blip_start
            print(f"BLIP-2 Model loaded ({blip_time:.2f}ì´ˆ)")

        self.init_time = time.time() - start_time
        self.clip_load_time = clip_time
        self.blip_load_time = blip_time
        print(f"ModelManager ì´ˆê¸°í™” ì™„ë£Œ (ì´ {self.init_time:.2f}ì´ˆ)")


    def get_clip_scores(self, images: List[Image.Image], text_queries: List[str]) -> np.ndarray:
        """
        Computes cosine similarity matrix between images and texts.
        Returns: (n_images, n_queries) numpy array
        """
        inputs = self.clip_processor(text=text_queries, images=images, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            outputs = self.clip_model(**inputs)
            # íŠ¹ì§• ë²¡í„°(Embedding)ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì„œ ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            image_embeds = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)
            text_embeds = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)
            cosine_sim = torch.matmul(image_embeds, text_embeds.T)
        return cosine_sim.cpu().numpy()

    def generate_caption(self, image: Image.Image) -> str:
        """Generates caption using BLIP-2"""
        if not self.use_blip:
            return ""
        inputs = self.blip_processor(images=image, return_tensors="pt").to(self.device, torch.float16)
        generated_ids = self.blip_model.generate(**inputs)
        return self.blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

    def get_text_features(self, text_list: List[str]):
        """í…ìŠ¤íŠ¸ë¥¼ CLIP ë²¡í„°ë¡œ ë³€í™˜ (í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë¹„êµìš©)"""
        inputs = self.clip_processor(text=text_list, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**inputs)
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def compute_text_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°"""
        feat1 = self.get_text_features([text1])
        feat2 = self.get_text_features([text2])
        sim = torch.matmul(feat1, feat2.T)
        return sim.item()

# ==========================================
# 2. Video Processor
# ==========================================

class VideoProcessor:
    def __init__(self, video_path, device="cuda"):
        start_time = time.time()
        self.video_path = video_path
        self.device = device
        
        # 1. VideoReader ì´ˆê¸°í™”
        # stream='video'ë¡œ ì„¤ì •í•˜ì—¬ ì˜¤ë””ì˜¤ ì œì™¸, ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ë§Œ íƒ€ê²ŸíŒ…
        self.v_reader = io.VideoReader(video_path, "video")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        self.metadata = self.v_reader.get_metadata()
        self.fps = self.metadata['video']['fps'][0]
        
        # duration ê³„ì‚° (ë°©ë²• A: ë©”íƒ€ë°ì´í„°ì—ì„œ ê³„ì‚°)
        # VideoReaderëŠ” ì´ í”„ë ˆì„ ìˆ˜ë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, 
        # ë¹„ë””ì˜¤ë¥¼ í•œ ë²ˆ ìˆœíšŒí•˜ì—¬ ê³„ì‚°
        self.total_frames = 0
        self.v_reader.seek(0)
        try:
            for _ in self.v_reader:
                self.total_frames += 1
        except StopIteration:
            pass
        
        # duration = ì´ í”„ë ˆì„ ìˆ˜ / FPS
        self.duration = self.total_frames / self.fps if self.fps > 0 else 0
        
        # ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¼
        self.v_reader.seek(0)
        
        self.init_time = time.time() - start_time
        print(f"âœ… VideoProcessor ë¡œë“œ ì™„ë£Œ (FPS: {self.fps}, ì´ í”„ë ˆì„: {self.total_frames}, Duration: {self.duration:.2f}ì´ˆ, ì´ˆê¸°í™” ì‹œê°„: {self.init_time:.2f}ì´ˆ)")

    def extract_window_frames(self, start_sec, end_sec, num_samples_q, window_idx=None, total_windows=None):
        """
        íŠ¹ì • êµ¬ê°„ì—ì„œ qê°œì˜ í”„ë ˆì„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ (224, 224)ë¡œ ë¦¬ì‚¬ì´ì§•
        
        Args:
            start_sec: ì‹œì‘ ì‹œê°„ (ì´ˆ)
            end_sec: ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
            num_samples_q: ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
            window_idx: í˜„ì¬ ìœˆë„ìš° ì¸ë±ìŠ¤ (ë¡œê¹…ìš©, optional)
            total_windows: ì „ì²´ ìœˆë„ìš° ìˆ˜ (ë¡œê¹…ìš©, optional)
        """
        frames = []
        # êµ¬ê°„ ë‚´ ê· ë“± ê°„ê²© ê³„ì‚°
        duration = end_sec - start_sec
        step = duration / max(1, (num_samples_q - 1))
        
        for i in range(num_samples_q):
            current_pos = start_sec + (i * step)
            
            # 2. ì •ë°€ íƒìƒ‰ (Seek)
            self.v_reader.seek(current_pos)
            
            try:
                # ë‹¤ìŒ í”„ë ˆì„ í•œ ì¥ ì½ê¸°
                frame_data = next(self.v_reader)
                
                if frame_data is not None:
                    if i == 0 and window_idx is not None:
                        print(f"  âœ… [Window {window_idx}/{total_windows}] [{start_sec:.1f}s] ì²« í”„ë ˆì„ ì½ê¸° ì„±ê³µ!")
                    
                    # frame_data['data']ëŠ” [C, H, W] í…ì„œ
                    img_tensor = frame_data['data'] # uint8 í…ì„œ
                    
                    # 3. ì¦‰ì‹œ ë¦¬ì‚¬ì´ì§• (ë©”ëª¨ë¦¬ ì ˆì•½ì˜ í•µì‹¬)
                    # PILë¡œ ë³€í™˜í•˜ê¸° ì „ì— í…ì„œ ìƒíƒœì—ì„œ (224, 224)ë¡œ ì¶•ì†Œ
                    resized_tensor = F.resize(img_tensor, [224, 224], antialias=True)
                    
                    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (CLIP ëª¨ë¸ ì…ë ¥ ê·œê²©)
                    img = Image.fromarray(resized_tensor.permute(1, 2, 0).byte().cpu().numpy())
                    frames.append(img)
                    
                    # ì‚¬ìš© ì¤‘ì¸ ì¤‘ê°„ í…ì„œ ëª…ì‹œì  ì‚­ì œ
                    del img_tensor
                    del resized_tensor
                    
            except StopIteration:
                print(f"  âš ï¸ [{current_pos:.1f}s] ì˜ìƒì˜ ëì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"  âŒ í”„ë ˆì„ ì¶”ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                continue
        
        return frames

    @staticmethod
    def clear_memory():
        """
        4. ë©”ëª¨ë¦¬ ê´€ë¦¬: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í…ì„œ ì •ë¦¬ ë° ìºì‹œ ë¹„ìš°ê¸°
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def get_timestamp_str(self, seconds):
        return str(datetime.timedelta(seconds=int(seconds)))


# ## 5. Real-time Visualization
# 
# ê²€ìƒ‰ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ì— í‘œì‹œí•˜ê³  ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.


# ==========================================
# 3. Real-time Visualization
# ==========================================
class RealTimeVisualizer:
    def __init__(self, total_duration, k_top, save_path="results"):
        """
        ì‹¤ì‹œê°„ ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤

        Args:
            total_duration: ë¹„ë””ì˜¤ ì´ ê¸¸ì´ (ì´ˆ)
            k_top: Top-K ê°œìˆ˜
            save_path: ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
        """
        self.total_duration = total_duration
        self.k_top = k_top
        self.save_path = save_path
        self.window_data = []
        self.current_top_k = []
        self.is_complete = False
        self.save_filename = None

        # ê·¸ë˜í”„ ì„¤ì •
        plt.ion()  # Interactive mode
        self.fig, self.ax = plt.subplots(figsize=(14, 6))
        self.fig.suptitle('Real-time Video Search Similarity Scores', fontsize=14, fontweight='bold')

    def update(self, window_info):
        """
        ìƒˆë¡œìš´ ìœˆë„ìš° ì •ë³´ë¡œ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸

        Args:
            window_info: {'start': float, 'end': float, 'clip_score_norm': float, 'is_top_k': bool}
        """
        self.window_data.append(window_info)

        # í˜„ì¬ê¹Œì§€ì˜ Top-K ê³„ì‚°
        sorted_windows = sorted(self.window_data, key=lambda x: x['clip_score_norm'], reverse=True)
        self.current_top_k = sorted_windows[:self.k_top]

        self._draw()

    def finalize(self, final_top_k):
        """
        ê²€ìƒ‰ ì™„ë£Œ í›„ ìµœì¢… Top-K í‘œì‹œ

        Args:
            final_top_k: ìµœì¢… Top-K ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸
        """
        self.is_complete = True
        self.final_top_k = final_top_k
        self._draw()

    def _draw(self):
        """ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
        self.ax.clear()

        if not self.window_data:
            return

        # ì‹œê°„ì¶•ê³¼ ì ìˆ˜ ë°ì´í„° ì¤€ë¹„
        times = [(w['start'] + w['end']) / 2 for w in self.window_data]
        scores = [w['clip_score_norm'] for w in self.window_data]

        # 1. ê¸°ë³¸ ì ìˆ˜ ì„  ê·¸ë˜í”„ (íšŒìƒ‰)
        self.ax.plot(times, scores, color='#CCCCCC', linewidth=1, alpha=0.6, zorder=1)

        # 2. ëª¨ë“  ìœˆë„ìš° ì  (ì‘ì€ íŒŒë€ìƒ‰)
        self.ax.scatter(times, scores, color='#4A90E2', s=30, alpha=0.5, zorder=2)

        # 3. í˜„ì¬ Top-K í›„ë³´ (ë…¸ë€ìƒ‰ í° ì )
        if not self.is_complete:
            top_k_times = [(w['start'] + w['end']) / 2 for w in self.current_top_k]
            top_k_scores = [w['clip_score_norm'] for w in self.current_top_k]
            self.ax.scatter(top_k_times, top_k_scores, color='#FFD700', s=200,
                          edgecolors='#FFA500', linewidths=2, zorder=4,
                          label=f'Current Top-{self.k_top}', marker='o', alpha=0.9)

            # ë°˜ì§ì´ëŠ” íš¨ê³¼ë¥¼ ìœ„í•œ ì™¸ê³½ì„ 
            for t, s in zip(top_k_times, top_k_scores):
                circle = Circle((t, s), radius=0.3, color='#FFD700', alpha=0.3, zorder=3)
                self.ax.add_patch(circle)

        # 4. ìµœì¢… Top-K (ë¹¨ê°„ìƒ‰ í° ì )
        if self.is_complete:
            final_times = [(w['start'] + w['end']) / 2 for w in self.final_top_k]
            final_scores = [w['clip_score_norm'] for w in self.final_top_k]
            self.ax.scatter(final_times, final_scores, color='#E74C3C', s=250,
                          edgecolors='#C0392B', linewidths=3, zorder=5,
                          label=f'Final Top-{self.k_top}', marker='*', alpha=1.0)

            # ìˆœìœ„ í‘œì‹œ
            for idx, (t, s, w) in enumerate(zip(final_times, final_scores, self.final_top_k), 1):
                self.ax.annotate(f'#{idx}', xy=(t, s), xytext=(5, 5),
                               textcoords='offset points', fontsize=10,
                               fontweight='bold', color='#E74C3C',
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='#E74C3C', alpha=0.8))

        # ê·¸ë˜í”„ ì„¤ì •
        self.ax.set_xlabel('Video Time (seconds)', fontsize=11, fontweight='bold')
        self.ax.set_ylabel('Normalized Similarity Score', fontsize=11, fontweight='bold')
        self.ax.set_xlim(0, self.total_duration)
        self.ax.set_ylim(0, 105)
        self.ax.grid(True, alpha=0.3, linestyle='--')
        self.ax.legend(loc='upper right', fontsize=9)

        # ì§„í–‰ë¥  í‘œì‹œ
        if self.window_data:
            progress = (self.window_data[-1]['end'] / self.total_duration) * 100
            status = "COMPLETE âœ“" if self.is_complete else f"Processing... {progress:.1f}%"
            self.ax.text(0.02, 0.98, status, transform=self.ax.transAxes,
                       fontsize=11, fontweight='bold', verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()
        plt.pause(0.01)

    def save_and_close(self, filename_base):
        """ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ê³  ì°½ ë‹«ê¸°"""
        plt.ioff()

        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_filename = f"viz_{filename_base}.png"
        save_path = os.path.join(self.save_path, self.save_filename)

        # ì´ë¯¸ì§€ë¡œ ì €ì¥
        self.fig.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  [ì‹œê°í™”] ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {self.save_filename}")

        # ì°½ ë‹«ê¸°
        plt.close(self.fig)

        return self.save_filename


# ## 6. Adaptive Search Engine (í•µì‹¬ ë¡œì§)
# 
# ì¿¼ë¦¬ ë¶„ì„, ë³€ê³¡ì  íƒì§€, CLIP/BLIP-2 ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ì—”ì§„ì…ë‹ˆë‹¤.
# 
# **ì£¼ìš” ê¸°ëŠ¥:**
# - Gemini APIë¥¼ í†µí•œ ì¿¼ë¦¬ ë¶„í• 
# - ì‹œí€€ì…œ ë™ì‘ ê°ì§€ (ë³€ê³¡ì  íƒì§€)
# - 2ë‹¨ê³„ ê²€ìƒ‰ (CLIP â†’ BLIP-2)
# - ì‹¤ì‹œê°„ ì‹œê°í™” ì§€ì›
# 
# > âš ï¸ ì´ ì…€ì€ ë§¤ìš° ê¸´ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤ (~500ì¤„)


# ==========================================
# 4. Adaptive Search Engine (Core Logic)
# ==========================================
class AdaptiveSearchEngine:
    def __init__(self, model_manager: ModelManager, video_processor: VideoProcessor):
        self.mm = model_manager
        self.vp = video_processor
        # íƒ€ì´ë° ì •ë³´ ì €ì¥
        self.timing_info = {
            "api_call_time": 0.0,
            "clip_inference_time": 0.0,
            "blip_inference_time": 0.0,
            "frame_extraction_time": 0.0,
            "total_search_time": 0.0
        }

    def _call_gemini_with_retry(self, prompt: str, max_retries: int = 3, timeout: int = 20) -> str:
        """
        Exponential Backoffê³¼ Jitterë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§ (Timeout ì ìš©)

        Args:
            prompt: Gemini APIì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            timeout: ê° ìš”ì²­ì˜ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        Returns:
            API ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        import signal
        from contextlib import contextmanager

        @contextmanager
        def time_limit(seconds):
            """íƒ€ì„ì•„ì›ƒ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
            def signal_handler(signum, frame):
                raise TimeoutError(f"API í˜¸ì¶œì´ {seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")

            # macOS/Linuxì—ì„œë§Œ signal ì‚¬ìš© ê°€ëŠ¥
            if hasattr(signal, 'SIGALRM'):
                signal.signal(signal.SIGALRM, signal_handler)
                signal.alarm(seconds)
                try:
                    yield
                finally:
                    signal.alarm(0)
            else:
                # Windowsì—ì„œëŠ” ë‹¨ìˆœ timeout
                yield

        # ì‹œë„í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
        models = [
            'models/gemini-2.0-flash-lite',
            'models/gemini-2.0-flash',
            'models/gemini-2.5-flash-lite'
        ]

        for model_idx, model in enumerate(models):
            for attempt in range(max_retries):
                try:
                    # Jitter ì¶”ê°€: 0.1~0.5ì´ˆ ëœë¤ ì§€ì—° (ë™ì‹œ ìš”ì²­ ì¶©ëŒ ë°©ì§€)
                    if attempt > 0:
                        jitter = random.uniform(0.1, 0.5)
                        time.sleep(jitter)

                    if attempt == 0 and model_idx == 0:
                        print(f"  [API] {model} í˜¸ì¶œ ì¤‘... (Timeout: {timeout}ì´ˆ)")
                    else:
                        print(f"  [API] ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëª¨ë¸: {model}, Timeout: {timeout}ì´ˆ)...")

                    # API í˜¸ì¶œ (Timeout ì ìš©)
                    start_time = time.time()
                    try:
                        if hasattr(signal, 'SIGALRM'):
                            with time_limit(timeout):
                                response = client.models.generate_content(
                                    model=model,
                                    contents=prompt
                                )
                        else:
                            # Windows: ë‹¨ìˆœ í˜¸ì¶œ
                            response = client.models.generate_content(
                                model=model,
                                contents=prompt
                            )
                            elapsed = time.time() - start_time
                            if elapsed > timeout:
                                raise TimeoutError(f"API í˜¸ì¶œì´ {timeout}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                    except TimeoutError as e:
                        print(f"  [API] â±ï¸ Timeout ({timeout}ì´ˆ ì´ˆê³¼). ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...")
                        break  # ë‹¤ìŒ ëª¨ë¸ë¡œ

                    elapsed = time.time() - start_time
                    print(f"  [API] âœ“ ì„±ê³µ! (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                    return response.text.strip()

                except Exception as e:
                    error_str = str(e)

                    # 429 Rate Limit ì—ëŸ¬ ì²˜ë¦¬
                    if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                        print(f"  [API] âš  Rate Limit ë„ë‹¬. ë‹¤ìŒ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì „í™˜...")
                        break  # ëŒ€ê¸°í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ëª¨ë¸ë¡œ

                    # ê¸°íƒ€ ì—ëŸ¬
                    else:
                        print(f"  [API] âœ— ì—ëŸ¬ ë°œìƒ: {error_str}")
                        if attempt < max_retries - 1:
                            continue
                        else:
                            if model_idx < len(models) - 1:
                                print(f"  [API] ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...")
                                break
                            else:
                                raise Exception(f"ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨: {error_str}")

        raise Exception("Gemini API í˜¸ì¶œ ì‹¤íŒ¨: ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")

    def split_query(self, text_query: str) -> tuple[list[str], str]:
        """
        Gemini APIë¥¼ í™œìš©í•˜ì—¬ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ë™ì‘ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, í•œêµ­ì–´ ì ‘ì†ì‚¬ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ì„ ì‹œë„í•©ë‹ˆë‹¤.

        Returns:
            tuple: (ë¶„í• ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ë¶„í•  ì´ìœ  ì„¤ëª…)
        """
        api_start_time = time.time()
        print(f"Thinking with Gemini (Korean Mode)... Query: '{text_query}'")

        # ---------------------------------------------------------
        # 1. Gemini API í˜¸ì¶œ (Primary Strategy)
        # ---------------------------------------------------------
        prompt = f"""
        ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ìœ„í•œ ì–¸ì–´ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´(Query)ê°€ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì—¬ëŸ¬ ë™ì‘(Sequence)ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´, ì´ë¥¼ ë¶„í• í•˜ì„¸ìš”.
        ê·¸ í›„, ê·¸ê²ƒë“¤ì„ CLIP ëª¨ë¸ì´ ê°€ì¥ ì˜ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì •êµí•œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­í•œ í›„ JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

        [ê·œì¹™]
        1. ë¬¸ë§¥ìƒ ì‹œê°„ì˜ íë¦„(ì˜ˆ: ~í•˜ê³  ë‚˜ì„œ, ~í•œ ë’¤ì—, ~í•˜ë‹¤ê°€)ì´ ìˆìœ¼ë©´ ë™ì‘ ë‹¨ìœ„ë¡œ ìª¼ê°œì„¸ìš”.
        2. ë‹¨ìˆœí•œ ë¬˜ì‚¬ë‚˜ ë‹¨ì¼ ë™ì‘ì´ë©´ ìš”ì†Œê°€ 1ê°œì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
        3. 1íšŒ ì´ìƒ ìª¼ê°œì§€ ë§ˆì„¸ìš”. (ìš”ì†ŒëŠ” 2ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.)
        4. ë°˜í™˜ê°’ì€ ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ê°ì²´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (Markdown ì œì™¸)
        5. ë¶„í• ëœ ë¬¸ì¥ì€ ê²€ìƒ‰ì´ ì˜ ë˜ë„ë¡ ê¸°ë³¸í˜•(ì˜ˆ: 'ë‹¬ë¦¬ê³ ' -> 'ë‹¬ë¦¬ëŠ” ì‚¬ëŒ')ì´ë‚˜ ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.
        6. ë¶„í•  ì—¬ë¶€ì™€ ê·¸ ì´ìœ ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.

        [ë°˜í™˜ í˜•ì‹]
        {{
        "results": [
            {{"ko": "í•œêµ­ì–´ ë™ì‘ 1", "en": "English description 1"}},
            {{"ko": "í•œêµ­ì–´ ë™ì‘ 2", "en": "English description 2"}}
        ],
        "reason": "ë¶„í•  ë° ë²ˆì—­ ì´ìœ "
        }}

        [ì˜ˆì‹œ 1]
        Query: "ê³µì„ ë˜ì§€ê³  ë‚˜ì„œ ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ"
        Output: {{
            "results": [
                {{"ko": "ê³µì„ ë˜ì§€ëŠ” ì‚¬ëŒ", "en": "Person throwing a ball"}},
                {{"ko": "ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ", "en": "Person falling down"}}
            ],
            "reason": "ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'ë˜ì§€ê³  ë‚˜ì„œ'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ê°œì˜ ì—°ì† ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
        }}

        [ì˜ˆì‹œ 2]
        Query: "ì›ƒê³  ìˆëŠ” ì•„ê¸°"
        Output: {{
            "results": [
                {{"ko": "ì›ƒê³  ìˆëŠ” ì•„ê¸°", "en": "Baby laughing"}}
            ],
            "reason": "ë‹¨ì¼ ë™ì‘/ìƒíƒœë¥¼ ë¬˜ì‚¬í•˜ëŠ” ì¿¼ë¦¬ë¡œ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }}

        [ì˜ˆì‹œ 3]
        Query: "ìš”ë¦¬ë¥¼ í•˜ë‹¤ê°€ ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì"
        Output: {{
            "results": [
                {{"ko": "ìš”ë¦¬ë¥¼ í•˜ëŠ” ë‚¨ì", "en": "Man cooking"}},
                {{"ko": "ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì", "en": "Man panicking after fire"}}
            ],
            "reason": "'í•˜ë‹¤ê°€'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ìˆœì„œìƒ ì„ í–‰ ë™ì‘ê³¼ í›„í–‰ ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
        }}

        [ì‹¤ì œ ì…ë ¥]
        Query: "{text_query}"
        """

        try:
            # API í˜¸ì¶œ (Retry ë¡œì§ í¬í•¨)
            result_text = self._call_gemini_with_retry(prompt)

            if result_text.startswith("```"):
                result_text = re.sub(r"```(json)?", "", result_text).strip()
                result_text = re.sub(r"```", "", result_text).strip()

            # JSON íŒŒì‹±
            result = json.loads(result_text)

            if isinstance(result, dict) and "results" in result:
                actions = [x["en"] for x in result["results"]]
                reason = result.get("reason", "ë¶„í•  ì™„ë£Œ")

                api_time = time.time() - api_start_time
                self.timing_info["api_call_time"] = api_time
                print(f" -> Gemini Split Result (EN): {actions} \n Reason: {reason}")
                print(f" -> API í˜¸ì¶œ ì‹œê°„: {api_time:.2f}ì´ˆ")
                return actions, f"[Gemini API] {reason}"

        except Exception as e:
            print(f"Gemini API Error: {e}. Switching to Fallback.")

        # ---------------------------------------------------------
        # 2. ê·œì¹™ ê¸°ë°˜ Fallback (í•œêµ­ì–´ ì ‘ì†ì‚¬ ì²˜ë¦¬)
        # ---------------------------------------------------------
        # APIê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‘ë‹µì´ ì´ìƒí•  ê²½ìš° ì‘ë™í•˜ëŠ” ë¹„ìƒ ë¡œì§ì…ë‹ˆë‹¤.
        # í•œêµ­ì–´ì—ì„œ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”í•œ í‘œí˜„ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ìë¦…ë‹ˆë‹¤.

        api_time = time.time() - api_start_time
        self.timing_info["api_call_time"] = api_time

        delimiters = [
            " ê·¸ë¦¬ê³  ", " ë‹¤ìŒì— ", " ê·¸ í›„ ", " ê·¸ ë’¤ì— ", " ë‚˜ì„œ ",
            " í•˜ë‹¤ê°€ ", "ë‹¤ê°€ "
        ]

        # ê°€ì¥ ë¨¼ì € ë°œê²¬ë˜ëŠ” êµ¬ë¶„ìë¡œ 1íšŒë§Œ ë¶„í•  ì‹œë„ (ë³µì¡ì„± ë°©ì§€)
        for delim in delimiters:
            if delim in text_query:
                parts = text_query.split(delim)
                # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ê³µë°± ì •ë¦¬
                clean_parts = [p.strip() for p in parts if p.strip()]
                if len(clean_parts) > 1:
                    print(f" -> Rule-based Split Result: {clean_parts}")
                    print(f" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ")
                    return clean_parts, f"[Rule-based] '{delim.strip()}' êµ¬ë¶„ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."

        # ë¶„í•  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        print("Final split result: ", [text_query])
        print(f" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ")
        return [text_query], "[Rule-based] ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ì´ ì—†ì–´ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    def calculate_sequential_score(self, frames, sub_queries):
        """
        [ë³€ê³¡ì  íƒì§€ ë¡œì§]
        ì¿¼ë¦¬ê°€ A -> Bë¡œ ë‚˜ë‰˜ì—ˆì„ ë•Œ, í”„ë ˆì„ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ìµœì ì˜ ë¶„í•  ì§€ì ì„ ì°¾ì•„
        (Aìœ ì‚¬ë„ + Bìœ ì‚¬ë„)ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Returns: (max_score, scores_matrix, best_split_index)
        """
        # (q_frames, 2_sub_queries) matrix
        scores_matrix = self.mm.get_clip_scores(frames, sub_queries)
        q_len = len(frames)
        max_score = -1.0
        best_split = -1

        # Linear Scan to find Change Point
        # ìµœì†Œ 20% ì§€ì ë¶€í„° 80% ì§€ì  ì‚¬ì´ì—ì„œ ë¶„í•  ì‹œë„
        start_idx = int(q_len * 0.2)
        end_idx = int(q_len * 0.8)

        if len(sub_queries) == 2:
            score_A = scores_matrix[:, 0] # Similarity curve for Query A
            score_B = scores_matrix[:, 1] # Similarity curve for Query B

            for t in range(start_idx, end_idx):
                # t ì‹œì ê¹Œì§€ëŠ” A, t ì´í›„ëŠ” B
                avg_A = np.mean(score_A[:t])
                avg_B = np.mean(score_B[t:])
                combined_score = (avg_A + avg_B) / 2

                if combined_score > max_score:
                    max_score = combined_score
                    best_split = t
        else:
            max_score = np.mean(np.max(scores_matrix, axis=1))

        return float(max_score), scores_matrix, best_split

    def normalize_score(self, raw_score: float) -> float:
        """
        CLIP ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ 0-100 ì ìˆ˜ë¡œ ë³€í™˜
        - 0.2 ì´í•˜: 0ì 
        - 0.45 ì´ìƒ: 100ì 
        """
        lower_bound = 0.20
        upper_bound = 0.45

        # ì •ê·œí™” ê³„ì‚°
        normalized = (raw_score - lower_bound) / (upper_bound - lower_bound) * 100

        # 0~100 ì‚¬ì´ë¡œ í´ë¦¬í•‘
        return float(np.clip(normalized, 0, 100))

    def search(self, original_query, sub_queries, p_sec, q_frames, k_top, weight_clip = 0.7, weight_semantic = 0.3, enable_visualization=True, save_path="results"):
        """
        Adaptive Search Engine ì‹¤í–‰ ë©”ì¸ ë¡œì§
        - 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)
        - 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)
        - 3. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ ë° ì •ë ¬

        Args:
            enable_visualization: ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™” ì—¬ë¶€
            save_path: ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
        """
        search_start_time = time.time()

        # íƒ€ì´ë° ì´ˆê¸°í™”
        total_frame_extraction_time = 0.0
        total_clip_inference_time = 0.0
        total_blip_inference_time = 0.0

        is_sequential = len(sub_queries) > 1

        all_windows = []
        step_size = p_sec  # ìœˆë„ìš°ê°€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ìˆ˜ì •
        current_time = 0.0

        # ì „ì²´ ìœˆë„ìš° ê°œìˆ˜ ê³„ì‚° (ì •í™•í•œ ê³„ì‚°)
        total_windows = int(np.ceil(self.vp.duration / step_size))

        # ì‹¤ì‹œê°„ ì‹œê°í™” ì´ˆê¸°í™”
        visualizer = None
        if enable_visualization:
            try:
                visualizer = RealTimeVisualizer(self.vp.duration, k_top, save_path)
                print(f"\n{'='*60}")
                print(f"[ğŸ“Š ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”] ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ì— í‘œì‹œí•©ë‹ˆë‹¤!")
                print(f"{'='*60}\n")
            except Exception as e:
                print(f"ì‹œê°í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì‹œê°í™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                visualizer = None

        print(f"\n{'='*60}")
        print(f"[ê²€ìƒ‰ ì‹œì‘] ì´ {total_windows}ê°œ ìœˆë„ìš° ì²˜ë¦¬ ì˜ˆì • (ìœˆë„ìš° í¬ê¸°: {p_sec}ì´ˆ, í”„ë ˆì„ ìƒ˜í”Œ: {q_frames}ê°œ)")
        print(f"{'='*60}\n")

        # 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)
        window_idx = 0
        current_top_window = None
        
        temp_thumb_dir = os.path.join(save_path, "temp_thumbs")
        if not os.path.exists(temp_thumb_dir):
            os.makedirs(temp_thumb_dir)
            
        while current_time < self.vp.duration:
            window_idx += 1
            end_time = min(current_time + p_sec, self.vp.duration)

            print(f"[Window {window_idx}/{total_windows}] ì²˜ë¦¬ ì¤‘: {self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}")

            # í”„ë ˆì„ ì¶”ì¶œ ì‹œê°„ ì¸¡ì •
            frame_start = time.time()
            frames = self.vp.extract_window_frames(current_time, end_time, q_frames, window_idx, total_windows)
            # ë””ë²„ê¹…: í”„ë ˆì„ ì¶”ì¶œ í™•ì¸
            if len(frames) > 0:
                frame_arr = np.array(frames[0])
                print(f"  [DEBUG] ì²« í”„ë ˆì„ í‰ê·  í”½ì…€ê°’: {frame_arr.mean():.2f}, í‘œì¤€í¸ì°¨: {frame_arr.std():.2f}")
            frame_time = time.time() - frame_start
            total_frame_extraction_time += frame_time

            # CLIP ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            clip_start = time.time()
            if is_sequential:
                raw_score, scores_matrix, best_split = self.calculate_sequential_score(frames, sub_queries)
                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥ (ì‹œí€€ì…œì˜ ê²½ìš° ë‘ ì¿¼ë¦¬ì— ëŒ€í•œ ì ìˆ˜)
                frame_scores = {
                    f"query_{i}": scores_matrix[:, i].tolist()
                    for i in range(len(sub_queries))
                }
                frame_scores["best_split_index"] = int(best_split) if best_split != -1 else None
            else:
                raw_scores_matrix = self.mm.get_clip_scores(frames, sub_queries)
                raw_score = float(np.mean(raw_scores_matrix))
                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥
                frame_scores = {
                    f"query_{i}": raw_scores_matrix[:, i].tolist()
                    for i in range(len(sub_queries))
                }
            clip_time = time.time() - clip_start
            total_clip_inference_time += clip_time

            clip_score_norm = self.normalize_score(raw_score)
            print(f"  -> ì •ê·œí™” CLIP ì ìˆ˜: {clip_score_norm:.2f} (í”„ë ˆì„ ì¶”ì¶œ: {frame_time:.2f}ì´ˆ, CLIP ì¶”ë¡ : {clip_time:.2f}ì´ˆ)")

            mid_img = frames[len(frames)//2]
            thumb_name = f"thumb_w{window_idx}_{current_time:.1f}.jpg"
            thumb_path = os.path.join(temp_thumb_dir, thumb_name)
            mid_img.save(thumb_path, "JPEG", quality=85) # íŒŒì¼ë¡œ ì €ì¥
            
            window_data = {
                "start": current_time,
                "end": end_time,
                "timestamp": f"{self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}",
                "raw_score": raw_score,           # ì°¸ê³ ìš© ì›ë³¸ ì ìˆ˜
                "clip_score_norm": clip_score_norm,    # ì •ê·œí™”ëœ ì ìˆ˜ (JSON ì €ì¥ìš©)
                "frame_scores": frame_scores,  # í”„ë ˆì„ë³„ ì ìˆ˜ ì¶”ê°€
                "temp_img_path": thumb_path,  # ê²½ë¡œë§Œ ì €ì¥ (RAM ì†Œëª¨ 0)
            }
            all_windows.append(window_data)
            
            del frames
            self.vp.clear_memory() # ë©”ëª¨ë¦¬ ì •ë¦¬

            # í˜„ì¬ê¹Œì§€ ìµœê³  ì ìˆ˜ ìœˆë„ìš° ì¶”ì 
            if current_top_window is None or clip_score_norm > current_top_window['clip_score_norm']:
                current_top_window = window_data
                print(f"  â­ ìƒˆë¡œìš´ Top ìœˆë„ìš° ë°œê²¬! ({current_top_window['timestamp']})\n")
            else:
                print(f"  [í˜„ì¬ Top] {current_top_window['timestamp']} (ì ìˆ˜: {current_top_window['clip_score_norm']:.4f})\n")

            # ì‹¤ì‹œê°„ ì‹œê°í™” ì—…ë°ì´íŠ¸
            if visualizer:
                try:
                    visualizer.update({
                        'start': current_time,
                        'end': end_time,
                        'clip_score_norm': clip_score_norm
                    })
                except Exception as e:
                    print(f"  [ì‹œê°í™” ê²½ê³ ] ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

            current_time += step_size

        # CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ Kê°œ ì„ ë³„
        print(f"\n{'='*60}")
        print(f"[1ì°¨ ê²€ìƒ‰ ì™„ë£Œ] CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ {k_top}ê°œ í›„ë³´ ì„ ë³„")
        print(f"{'='*60}")

        all_windows.sort(key=lambda x: x["clip_score_norm"], reverse=True)
        top_k_candidates = all_windows[:k_top]

        for idx, item in enumerate(top_k_candidates, 1):
            print(f"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}")

        # 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)
        if self.mm.use_blip:
            print(f"\n{'='*60}")
            print(f"[2ì°¨ ë³´ì • ì‹œì‘] BLIP-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ {k_top}ê°œ í›„ë³´ ë³´ì • ì¤‘...")
            print(f"{'='*60}\n")

            for idx, item in enumerate(top_k_candidates, 1):
                print(f"[í›„ë³´ {idx}/{k_top}] {item['timestamp']}")

                # A. BLIP-2ë¡œ í”„ë ˆì„ ì„¤ëª…(Caption) ìƒì„± - ì‹œê°„ ì¸¡ì •
                blip_start = time.time()
                img_for_blip = Image.open(item['temp_img_path'])
                item['blip_caption'] = self.mm.generate_caption(img_for_blip)
                blip_time = time.time() - blip_start
                total_blip_inference_time += blip_time

                del img_for_blip

                # B. ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ìƒì„±ëœ ìº¡ì…˜ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (Text-to-Text)
                semantic_start = time.time()
                semantic_sim = self.mm.compute_text_similarity(original_query, item['blip_caption'])
                semantic_time = time.time() - semantic_start
                total_blip_inference_time += semantic_time

                item['semantic_consistency'] = semantic_sim

                # C. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ (ì•™ìƒë¸”)
                item['final_score'] = (item['clip_score_norm'] * weight_clip) + (semantic_sim * weight_semantic)

                print(f"  -> ìƒì„±ëœ ìº¡ì…˜: {item['blip_caption']}")
                print(f"  -> ì˜ë¯¸ ìœ ì‚¬ë„: {semantic_sim:.4f}")
                print(f"  -> ìµœì¢… ì ìˆ˜: {item['final_score']:.4f}")
                print(f"  -> BLIP-2 ì²˜ë¦¬ ì‹œê°„: {blip_time + semantic_time:.2f}ì´ˆ\n")

            # ë³´ì •ëœ ìµœì¢… ì ìˆ˜ë¡œ ë‹¤ì‹œ ì •ë ¬
            top_k_candidates.sort(key=lambda x: x.get('final_score', x['clip_score_norm']), reverse=True)

            print(f"{'='*60}")
            print(f"[ìµœì¢… ìˆœìœ„]")
            print(f"{'='*60}")
            for idx, item in enumerate(top_k_candidates, 1):
                print(f"{idx}. {item['timestamp']} - ìµœì¢… ì ìˆ˜: {item.get('final_score', item['clip_score_norm']):.4f}")
        else:
            # BLIP-2 ì—†ì„ ë•Œë„ ìµœì¢… ìˆœìœ„ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"[ìµœì¢… ìˆœìœ„]")
            print(f"{'='*60}")
            for idx, item in enumerate(top_k_candidates, 1):
                print(f"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}")

        print()

        # ì‹¤ì‹œê°„ ì‹œê°í™” ìµœì¢… ì—…ë°ì´íŠ¸ (BLIP-2 ë³´ì • ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ì—¬ê¸°ì„œ í˜¸ì¶œ)
        if visualizer:
            try:
                visualizer.finalize(top_k_candidates)
                print(f"\n{'='*60}")
                print(f"[ğŸ“Š ì‹œê°í™” ì™„ë£Œ] ìµœì¢… Top-{k_top} ê²°ê³¼ê°€ ë¹¨ê°„ìƒ‰ ë³„(â˜…)ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"{'='*60}\n")
            except Exception as e:
                print(f"[ì‹œê°í™” ì˜¤ë¥˜] ìµœì¢… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

        # íƒ€ì´ë° ì •ë³´ ì €ì¥
        total_search_time = time.time() - search_start_time
        self.timing_info["frame_extraction_time"] = total_frame_extraction_time
        self.timing_info["clip_inference_time"] = total_clip_inference_time
        self.timing_info["blip_inference_time"] = total_blip_inference_time
        self.timing_info["total_search_time"] = total_search_time

        # visualizer ê°ì²´ë¥¼ ë°˜í™˜ (mainì—ì„œ ì €ì¥)
        return top_k_candidates, all_windows, visualizer

# # ==========================================
# # ëª¨ë¸ ì´ˆê¸°í™” (ì´ ì…€ì€ ëŸ°íƒ€ì„ ì‹œì‘ ì‹œ 1ë²ˆë§Œ ì‹¤í–‰!)
# # ==========================================
# print("ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸ ì¤‘...")

# # Configuration
# USE_BLIP = True  # BLIP-2 ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì£¼ì˜)

# # Initialize (ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥)
# # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
# if 'model_manager' not in globals():
#     print("ğŸ”„ ModelManager ì´ˆê¸°í™” ì¤‘...")
#     init_start_time = time.time()
#     model_manager = ModelManager(use_blip=USE_BLIP)
#     print(f"âœ… ModelManager ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - init_start_time:.2f}ì´ˆ)")
# else:
#     print("â™»ï¸ ê¸°ì¡´ ModelManager ì¬ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½!)")

# # USE_BLIP ì‚¬ìš© ì—¬ë¶€ ë³€ê²½í–ˆì„ ê²½ìš°ì—ëŠ” 
# # ìœ„ ì½”ë“œ ë¸”ëŸ­ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ì•„ë˜ ì½”ë“œ ì‹¤í–‰
# # model_manager = ModelManager(use_blip=USE_BLIP)

# print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ! ì´ì œ ì•„ë˜ ì‹¤í–‰ ì…€ì„ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ëª¨ë¸ì´ ë‹¤ì‹œ ë¡œë“œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def cleanup_temp_images(path):
    """ì„ì‹œ ì¸ë„¤ì¼ í´ë” ì‚­ì œ"""
    temp_path = os.path.join(path, "temp_thumbs")
    if os.path.exists(temp_path):
        shutil.rmtree(temp_path)
        print(f"ğŸ§¹ ì„ì‹œ ì´ë¯¸ì§€ í´ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: {temp_path}")

def collect_timing_data(total_elapsed_time, total_init_time, model_manager, video_processor, engine):
    """
    íƒ€ì´ë° ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    
    Args:
        total_elapsed_time: ì „ì²´ ì‹¤í–‰ ì‹œê°„
        total_init_time: ì´ˆê¸°í™” ì‹œê°„
        model_manager: ModelManager ì¸ìŠ¤í„´ìŠ¤
        video_processor: VideoProcessor ì¸ìŠ¤í„´ìŠ¤
        engine: AdaptiveSearchEngine ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        dict: íƒ€ì´ë° ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    return {
        "total_time": round(total_elapsed_time, 2),
        "init_time": round(total_init_time, 2),
        "model_manager_init_time": round(model_manager.init_time, 2),
        "clip_load_time": round(model_manager.clip_load_time, 2),
        "blip_load_time": round(model_manager.blip_load_time, 2),
        "video_processor_init_time": round(video_processor.init_time, 2),
        "api_call_time": round(engine.timing_info["api_call_time"], 2),
        "frame_extraction_time": round(engine.timing_info["frame_extraction_time"], 2),
        "clip_inference_time": round(engine.timing_info["clip_inference_time"], 2),
        "blip_inference_time": round(engine.timing_info["blip_inference_time"], 2),
        "total_search_time": round(engine.timing_info["total_search_time"], 2)
    }

# ==========================================
# 5. Main Execution
# ==========================================
def main():
    # ì „ì—­ ë³€ìˆ˜ ì„ ì–¸ (í•¨ìˆ˜ ì•ˆì—ì„œ ì „ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©/ìˆ˜ì •í•˜ê¸° ìœ„í•´ í•„ìš”)
    global model_manager, video_processor
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
    program_start_time = time.time()

    # --- Configurations ---
    VIDEO_PATH = "sample_video.mp4" # ì¤€ë¹„ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    SAVE_PATH = "results"
    USE_BLIP = True  # BLIP-2 ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì£¼ì˜)
    if not os.path.exists(SAVE_PATH):
        os.makedirs(SAVE_PATH)
    else:
        print(f"Save path '{SAVE_PATH}' already exists. Results will be saved here.")
    QUERY = "ë°”ë‹¥ì— ë–¨ì–´ì§„ ì‹ ìš©ì¹´ë“œ"
    # "ë°”ë‹¥ì— ë–¨ì–´ì§€ëŠ” ì¹´ë“œë¥¼ ë³´ê³  ë‚œê°í•œ í‘œì •ì„ ì§“ëŠ” ë‚¨ì" # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬

    # Experiments Parameters
    p_list = [2.0, 4.0]      # ìœˆë„ìš° í¬ê¸° (ì´ˆ)
    q_list = [12, 24, 48]         # ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜
    k_list = [3, 5]          # Top-K ê°œìˆ˜
    WEIGHT_CLIP = 0.7
    WEIGHT_SEMANTIC = 0.3
    USE_LOOP = False         # ë°˜ë³µ ì‹¤í–‰ ì—¬ë¶€

    # Initialize
    if not os.path.exists(VIDEO_PATH):
        print(f"Error: Video file '{VIDEO_PATH}' not found. Please place a dummy video.")
        return

    # ì´ˆê¸°í™” ì‹œê°„ ì¸¡ì •
    init_start_time = time.time()
    # Initialize (ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥)
    # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
    
    # main() í•¨ìˆ˜ ì•ˆ (1001ì¤„ ì•ì— ì¶”ê°€)
    print(f"[DEBUG] 'model_manager' in globals(): {'model_manager' in globals()}")
    if 'model_manager' in globals():
        print(f"[DEBUG] model_manager is None: {model_manager is None}")
        print(f"[DEBUG] model_manager ê°’: {model_manager}")

    if 'model_manager' not in globals() or model_manager is None:
        print("ğŸ”„ ModelManager ì´ˆê¸°í™” ì¤‘...")
        init_start_time = time.time()
        model_manager = ModelManager(use_blip=USE_BLIP)
        print(f"âœ… ModelManager ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - init_start_time:.2f}ì´ˆ)")
    else:
        print("â™»ï¸ ê¸°ì¡´ ModelManager ì¬ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½!)")
    if 'video_processor' not in globals() or video_processor is None:
        print("ğŸ”„ VideoProcessor ì´ˆê¸°í™” ì¤‘...")
        video_processor = VideoProcessor(VIDEO_PATH)
        print(f"âœ… VideoProcessor ì´ˆê¸°í™” ì™„ë£Œ ({time.time() - init_start_time:.2f}ì´ˆ)")
    else:
        print("â™»ï¸ ê¸°ì¡´ VideoProcessor ì¬ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½!)")
    engine = AdaptiveSearchEngine(model_manager, video_processor)
    total_init_time = time.time() - init_start_time

    print(f"\n[ì¿¼ë¦¬ ë¶„ì„] '{QUERY}'")
    sub_queries, split_reason = engine.split_query(QUERY)
    print(f"[ë¶„í• ëœ ì¿¼ë¦¬] {sub_queries}\n")

    # Experiment Loop
    # ë°˜ë³µ ì‹¤í–‰ í•  ë•Œ
    if USE_LOOP:
        for p in p_list:
            for q in q_list:
                for k in k_list:
                    print(f"\n--- Running Experiment: p={p}, q={q}, k={k} ---")

                    # Perform Search (ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”)
                    results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p, q, k, WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)

                    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                    total_elapsed_time = time.time() - program_start_time

                    # Construct Filename
                    model_name = "CB" if USE_BLIP else "Clip"
                    timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    # ìœ ì˜ë¯¸í•œ ê²°ê³¼ ë‚˜ì™”ìœ¼ë©´ _test.json ëŒ€ì‹  .json í™•ì¥ì ì‚¬ìš©
                    filename = f"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json"
                    filename_base = f"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test"

                    # ì‹œê°í™” ì €ì¥
                    if visualizer:
                        try:
                            viz_filename = visualizer.save_and_close(filename_base)
                        except Exception as e:
                            print(f"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}")
                            viz_filename = None
                    else:
                        viz_filename = None

                    # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
                    timing_data = collect_timing_data(total_elapsed_time, total_init_time, 
                                                    model_manager, video_processor, engine)

                    # Output Data Structure
                    output_data = {
                        "meta": {
                            "video_path": VIDEO_PATH,
                            "query": QUERY,
                            "sub_queries": sub_queries,
                            "split_reason": split_reason,
                            "parameters": {"p": p, "q": q, "k": k, "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                            "model": model_name,
                            "timestamp": timestamp_str
                        },
                        "time_used": timing_data,
                        "results": results
                    }

                    # Save to JSON
                    with open(os.path.join(SAVE_PATH, filename), "w", encoding='utf-8') as f:
                        json.dump(output_data, f, indent=4, ensure_ascii=False)

                    # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥
                    whole_score_filename = f"whole_score_{filename}"
                    whole_score_data = {
                        "meta": {
                            "video_path": VIDEO_PATH,
                            "query": QUERY,
                            "sub_queries": sub_queries,
                            "parameters": {"p": p, "q": q, "k": k, "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                            "total_windows": len(all_windows_data),
                            "timestamp": timestamp_str
                        },
                        "all_windows": all_windows_data
                    }
                    with open(os.path.join(SAVE_PATH, whole_score_filename), "w", encoding='utf-8') as f:
                        json.dump(whole_score_data, f, indent=4, ensure_ascii=False)

                    print(f"\n[ì €ì¥ ì™„ë£Œ] {filename}")
                    print(f"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}")
                    print(f"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨")
                    if viz_filename:
                        print(f"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}")
                    print(f"[ì´ ì‹¤í–‰ ì‹œê°„] {total_elapsed_time:.2f}ì´ˆ\n")
                    cleanup_temp_images(SAVE_PATH)

    # ë°˜ë³µ ì‹¤í–‰ ì•„ë‹ ë•Œ
    else:
        results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p_list[0], q_list[0], k_list[0], WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)

        # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        total_elapsed_time = time.time() - program_start_time

        model_name = "CB" if USE_BLIP else "Clip"
        timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json"
        filename_base = f"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test"

        # ì‹œê°í™” ì €ì¥
        if visualizer:
            try:
                viz_filename = visualizer.save_and_close(filename_base)
            except Exception as e:
                print(f"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}")
                viz_filename = None
        else:
            viz_filename = None

        # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
        timing_data = collect_timing_data(total_elapsed_time, total_init_time, 
                                        model_manager, video_processor, engine)

        # Output Data Structure
        output_data = {
            "meta": {
                "video_path": VIDEO_PATH,
                "query": QUERY,
                "sub_queries": sub_queries,
                "split_reason": split_reason,
                "parameters": {"p": p_list[0], "q": q_list[0], "k": k_list[0], "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                "model": model_name,
                "timestamp": timestamp_str
            },
            "time_used": timing_data,
            "results": results
        }

        # Save to JSON
        with open(os.path.join(SAVE_PATH, filename), "w", encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)

        # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥
        whole_score_filename = f"whole_score_{filename}"
        whole_score_data = {
            "meta": {
                "video_path": VIDEO_PATH,
                "query": QUERY,
                "sub_queries": sub_queries,
                "parameters": {"p": p_list[0], "q": q_list[0], "k": k_list[0], "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                "total_windows": len(all_windows_data),
                "timestamp": timestamp_str
            },
            "all_windows": all_windows_data
        }
        with open(os.path.join(SAVE_PATH, whole_score_filename), "w", encoding='utf-8') as f:
            json.dump(whole_score_data, f, indent=4, ensure_ascii=False)

        print(f"\n{'='*60}")
        print(f"[ê²€ìƒ‰ ì™„ë£Œ] ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}")
        print(f"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨")
        if viz_filename:
            print(f"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}")
        print(f"{'='*60}")
        print(f"\nğŸ“Š [ì „ì²´ ì‹¤í–‰ ì‹œê°„ ë¶„ì„]")
        print(f"{'='*60}")
        print(f"  â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        print(f"\n  ğŸ”§ ì´ˆê¸°í™” ë‹¨ê³„:")
        print(f"     - ModelManager ì´ˆê¸°í™”: {model_manager.init_time:.2f}ì´ˆ")
        print(f"       â”œâ”€ CLIP ë¡œë“œ: {model_manager.clip_load_time:.2f}ì´ˆ")
        print(f"       â””â”€ BLIP-2 ë¡œë“œ: {model_manager.blip_load_time:.2f}ì´ˆ")
        print(f"     - VideoProcessor ì´ˆê¸°í™”: {video_processor.init_time:.2f}ì´ˆ")
        print(f"     - ì „ì²´ ì´ˆê¸°í™”: {total_init_time:.2f}ì´ˆ")
        print(f"\n  ğŸ” ê²€ìƒ‰ ë‹¨ê³„:")
        print(f"     - API í˜¸ì¶œ (ì¿¼ë¦¬ ë¶„ì„): {engine.timing_info['api_call_time']:.2f}ì´ˆ")
        print(f"     - í”„ë ˆì„ ì¶”ì¶œ: {engine.timing_info['frame_extraction_time']:.2f}ì´ˆ")
        print(f"     - CLIP ì¶”ë¡ : {engine.timing_info['clip_inference_time']:.2f}ì´ˆ")
        if USE_BLIP:
            print(f"     - BLIP-2 ì¶”ë¡ : {engine.timing_info['blip_inference_time']:.2f}ì´ˆ")
        print(f"     - ì „ì²´ ê²€ìƒ‰: {engine.timing_info['total_search_time']:.2f}ì´ˆ")
        print(f"{'='*60}\n")
        cleanup_temp_images(SAVE_PATH)

if __name__ == "__main__":
    main()


# ## 8. ì‹¤í–‰ ì˜ˆì œ
# 
# ìœ„ì˜ main() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


# ì£¼ì„ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”
# main()


