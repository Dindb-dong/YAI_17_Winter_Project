{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "093rGfmHLwtq"
      },
      "source": [
        "# ğŸ¬ Adaptive Video Search Engine\n",
        "\n",
        "**ìì—°ì–´ ì¿¼ë¦¬ ê¸°ë°˜ ì§€ëŠ¥í˜• ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹œìŠ¤í…œ**\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ CLIP, BLIP-2, Gemini APIë¥¼ í™œìš©í•œ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì—”ì§„ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥\n",
        "1. **ì§€ëŠ¥í˜• ì¿¼ë¦¬ ë¶„ì„**: Gemini APIë¡œ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ë™ì‘ ì‹œí€€ìŠ¤ë¡œ ë¶„í• \n",
        "2. **ì ì‘í˜• ë§¤ì¹­ ì—”ì§„**: CLIP ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° + ë³€ê³¡ì  íƒì§€\n",
        "3. **2ë‹¨ê³„ ê²€ìƒ‰**: CLIP â†’ BLIP-2 ë³´ì •ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ\n",
        "4. **ì‹¤ì‹œê°„ ì‹œê°í™”**: ê²€ìƒ‰ ì§„í–‰ ìƒí™©ì„ ê·¸ë˜í”„ë¡œ í‘œì‹œ\n",
        "\n",
        "## ğŸ“¦ ì‹œì‘í•˜ê¸° ì „ì—\n",
        "- GPU ì‚¬ìš© ê¶Œì¥ (BLIP-2 ì‚¬ìš© ì‹œ)\n",
        "- `.env` íŒŒì¼ì— `GEMINI_API_KEY` ì„¤ì • í•„ìš”\n",
        "- ë¹„ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ (`sample_video.mp4`)"
      ],
      "id": "093rGfmHLwtq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-JeATLNLwtr"
      },
      "source": [
        "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "\n",
        "ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
      ],
      "id": "c-JeATLNLwtr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9cb8863",
      "metadata": {
        "id": "d9cb8863",
        "outputId": "5322101d-4204-4985-c1c9-3d827843565b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "í•„ìš”í•œ íŒ¨í‚¤ì§€: torch, transformers, opencv-python, pillow, numpy, google-genai, python-dotenv, matplotlib\n",
            "ìœ„ ì£¼ì„ì˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers opencv-python pillow numpy google-genai python-dotenv matplotlib\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install triton"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a069844",
      "metadata": {
        "id": "1a069844"
      },
      "source": [
        "## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad42559",
      "metadata": {
        "id": "5ad42559"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, Blip2Processor, Blip2ForConditionalGeneration\n",
        "from typing import List, Dict, Union, Tuple\n",
        "from google import genai\n",
        "import json\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0beec004",
      "metadata": {
        "id": "0beec004"
      },
      "source": [
        "## 3. Gemini API ì„¤ì • & Model Manager\n",
        "\n",
        "Gemini API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³ , CLIP ë° BLIP-2 ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd5855a",
      "metadata": {
        "id": "3fd5855a"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Gemini API ì„¤ì • (AdaptiveSearchEngine ë‚´ë¶€ í˜¹ì€ ì™¸ë¶€ì— ì„ ì–¸)\n",
        "# ==========================================\n",
        "# .env íŒŒì¼ ë¡œë“œ\n",
        "load_dotenv()\n",
        "\n",
        "# API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "client = genai.Client(api_key=GEMINI_API_KEY, http_options=genai.types.HttpOptions(api_version=\"v1\"))\n",
        "if client is not None:\n",
        "    print(\"Gemini Client initialized successfully\")\n",
        "else:\n",
        "    print(\"Gemini Client initialization failed\")\n",
        "    exit()\n",
        "# # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
        "# for model in client.models.list():\n",
        "#     print(f\"Model Name: {model.name}, Supported Methods: {model.supported_actions}\")\n",
        "# exit()\n",
        "\n",
        "# ==========================================\n",
        "# 1. Model Manager (CLIP & BLIP-2)\n",
        "# ==========================================\n",
        "class ModelManager:\n",
        "    def __init__(self, use_blip=False, device=None):\n",
        "        start_time = time.time()\n",
        "        print(\"Initializing ModelManager...\")\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Loading models on {self.device}...\")\n",
        "\n",
        "        # Load CLIP (Base Model)\n",
        "        clip_start = time.time()\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\n",
        "            \"openai/clip-vit-base-patch32\",\n",
        "            use_fast=True  # ì´ ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ Rust ê¸°ë°˜ì˜ ë¹ ë¥¸ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "        )\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
        "        self.clip_model.eval()\n",
        "        clip_time = time.time() - clip_start\n",
        "        print(f\"CLIP Model loaded ({clip_time:.2f}ì´ˆ)\")\n",
        "\n",
        "        # Load BLIP-2 (Refinement Model) - Optional\n",
        "        self.use_blip = use_blip\n",
        "        self.blip_processor = None\n",
        "        self.blip_model = None\n",
        "        blip_time = 0.0\n",
        "\n",
        "        if self.use_blip:\n",
        "            print(\"Loading BLIP-2 (this might take memory)...\")\n",
        "            blip_start = time.time()\n",
        "            self.blip_processor = Blip2Processor.from_pretrained(\n",
        "                \"Salesforce/blip2-opt-2.7b\",\n",
        "                use_fast=True)\n",
        "            self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "                \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
        "            ).to(self.device)\n",
        "            self.blip_model.eval()\n",
        "            blip_time = time.time() - blip_start\n",
        "            print(f\"BLIP-2 Model loaded ({blip_time:.2f}ì´ˆ)\")\n",
        "\n",
        "        self.init_time = time.time() - start_time\n",
        "        self.clip_load_time = clip_time\n",
        "        self.blip_load_time = blip_time\n",
        "        print(f\"ModelManager ì´ˆê¸°í™” ì™„ë£Œ (ì´ {self.init_time:.2f}ì´ˆ)\")\n",
        "\n",
        "\n",
        "    def get_clip_scores(self, images: List[Image.Image], text_queries: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes cosine similarity matrix between images and texts.\n",
        "        Returns: (n_images, n_queries) numpy array\n",
        "        \"\"\"\n",
        "        inputs = self.clip_processor(text=text_queries, images=images, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.clip_model(**inputs)\n",
        "            # íŠ¹ì§• ë²¡í„°(Embedding)ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì„œ ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            image_embeds = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "            text_embeds = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)\n",
        "            cosine_sim = torch.matmul(image_embeds, text_embeds.T)\n",
        "        return cosine_sim.cpu().numpy()\n",
        "\n",
        "    def generate_caption(self, image: Image.Image) -> str:\n",
        "        \"\"\"Generates caption using BLIP-2\"\"\"\n",
        "        if not self.use_blip:\n",
        "            return \"\"\n",
        "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device, torch.float16)\n",
        "        generated_ids = self.blip_model.generate(**inputs)\n",
        "        return self.blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    def get_text_features(self, text_list: List[str]):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ CLIP ë²¡í„°ë¡œ ë³€í™˜ (í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë¹„êµìš©)\"\"\"\n",
        "        inputs = self.clip_processor(text=text_list, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_features = self.clip_model.get_text_features(**inputs)\n",
        "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def compute_text_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "        feat1 = self.get_text_features([text1])\n",
        "        feat2 = self.get_text_features([text2])\n",
        "        sim = torch.matmul(feat1, feat2.T)\n",
        "        return float(sim.cpu().numpy())\n",
        "\n",
        "print(\"âœ… ModelManager í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45956f52",
      "metadata": {
        "id": "45956f52"
      },
      "source": [
        "## 4. Video Processor\n",
        "\n",
        "ë¹„ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ìœˆë„ìš° ê¸°ë°˜ìœ¼ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56edbc9b",
      "metadata": {
        "id": "56edbc9b"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. Video Processor\n",
        "# ==========================================\n",
        "class VideoProcessor:\n",
        "    def __init__(self, video_path):\n",
        "        start_time = time.time()\n",
        "        self.video_path = video_path\n",
        "        self.cap = cv2.VideoCapture(video_path)\n",
        "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
        "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        self.duration = self.total_frames / self.fps\n",
        "        self.init_time = time.time() - start_time\n",
        "        print(f\"Video processor initialized ({self.init_time:.2f}ì´ˆ)\")\n",
        "    def extract_window_frames(self, start_sec, end_sec, num_samples_q, window_idx=None, total_windows=None) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Extracts 'q' frames uniformly from the window [start_sec, end_sec].\n",
        "        \"\"\"\n",
        "        frames = []\n",
        "        start_frame = int(start_sec * self.fps)\n",
        "        end_frame = int(end_sec * self.fps)\n",
        "\n",
        "        # Uniform sampling indices\n",
        "        indices = np.linspace(start_frame, end_frame - 1, num_samples_q, dtype=int)\n",
        "\n",
        "        for frame_num, idx in enumerate(indices, 1):\n",
        "            if window_idx is not None and total_windows is not None:\n",
        "                print(f\"  [Window {window_idx}/{total_windows}] í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {frame_num}/{num_samples_q}\", end='\\r')\n",
        "            self.cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "            ret, frame = self.cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(Image.fromarray(frame))\n",
        "            else:\n",
        "                # Padding with black frame if read fails\n",
        "                frames.append(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
        "\n",
        "        if window_idx is not None:\n",
        "            print()  # ì¤„ë°”ê¿ˆ\n",
        "        return frames\n",
        "\n",
        "    def get_timestamp_str(self, seconds):\n",
        "        return str(datetime.timedelta(seconds=int(seconds)))\n",
        "\n",
        "print(\"âœ… VideoProcessor í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bfed17b",
      "metadata": {
        "id": "0bfed17b"
      },
      "source": [
        "## 5. Adaptive Search Engine (í•µì‹¬ ë¡œì§)\n",
        "\n",
        "ì¿¼ë¦¬ ë¶„ì„, ë³€ê³¡ì  íƒì§€, CLIP/BLIP-2 ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ì—”ì§„ì…ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ê¸°ëŠ¥:**\n",
        "- Gemini APIë¥¼ í†µí•œ ì¿¼ë¦¬ ë¶„í• \n",
        "- ì‹œí€€ì…œ ë™ì‘ ê°ì§€ (ë³€ê³¡ì  íƒì§€)\n",
        "- 2ë‹¨ê³„ ê²€ìƒ‰ (CLIP â†’ BLIP-2)\n",
        "- ì‹¤ì‹œê°„ ì‹œê°í™” ì§€ì›\n",
        "\n",
        "> âš ï¸ ì´ ì…€ì€ ë§¤ìš° ê¸´ ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤ (~500ì¤„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b49a7f5",
      "metadata": {
        "id": "9b49a7f5"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Adaptive Search Engine (Core Logic)\n",
        "# ==========================================\n",
        "class AdaptiveSearchEngine:\n",
        "    def __init__(self, model_manager: ModelManager, video_processor: VideoProcessor):\n",
        "        self.mm = model_manager\n",
        "        self.vp = video_processor\n",
        "        # íƒ€ì´ë° ì •ë³´ ì €ì¥\n",
        "        self.timing_info = {\n",
        "            \"api_call_time\": 0.0,\n",
        "            \"clip_inference_time\": 0.0,\n",
        "            \"blip_inference_time\": 0.0,\n",
        "            \"frame_extraction_time\": 0.0,\n",
        "            \"total_search_time\": 0.0\n",
        "        }\n",
        "\n",
        "    def _call_gemini_with_retry(self, prompt: str, max_retries: int = 3, timeout: int = 20) -> str:\n",
        "        \"\"\"\n",
        "        Exponential Backoffê³¼ Jitterë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§ (Timeout ì ìš©)\n",
        "\n",
        "        Args:\n",
        "            prompt: Gemini APIì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸\n",
        "            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\n",
        "            timeout: ê° ìš”ì²­ì˜ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)\n",
        "\n",
        "        Returns:\n",
        "            API ì‘ë‹µ í…ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        import signal\n",
        "        from contextlib import contextmanager\n",
        "\n",
        "        @contextmanager\n",
        "        def time_limit(seconds):\n",
        "            \"\"\"íƒ€ì„ì•„ì›ƒ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €\"\"\"\n",
        "            def signal_handler(signum, frame):\n",
        "                raise TimeoutError(f\"API í˜¸ì¶œì´ {seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "            # macOS/Linuxì—ì„œë§Œ signal ì‚¬ìš© ê°€ëŠ¥\n",
        "            if hasattr(signal, 'SIGALRM'):\n",
        "                signal.signal(signal.SIGALRM, signal_handler)\n",
        "                signal.alarm(seconds)\n",
        "                try:\n",
        "                    yield\n",
        "                finally:\n",
        "                    signal.alarm(0)\n",
        "            else:\n",
        "                # Windowsì—ì„œëŠ” ë‹¨ìˆœ timeout\n",
        "                yield\n",
        "\n",
        "        # ì‹œë„í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ ìˆœ)\n",
        "        models = [\n",
        "            'models/gemini-2.0-flash-lite',\n",
        "            'models/gemini-2.0-flash',\n",
        "            'models/gemini-2.5-flash-lite'\n",
        "        ]\n",
        "\n",
        "        for model_idx, model in enumerate(models):\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    # Jitter ì¶”ê°€: 0.1~0.5ì´ˆ ëœë¤ ì§€ì—° (ë™ì‹œ ìš”ì²­ ì¶©ëŒ ë°©ì§€)\n",
        "                    if attempt > 0:\n",
        "                        jitter = random.uniform(0.1, 0.5)\n",
        "                        time.sleep(jitter)\n",
        "\n",
        "                    if attempt == 0 and model_idx == 0:\n",
        "                        print(f\"  [API] {model} í˜¸ì¶œ ì¤‘... (Timeout: {timeout}ì´ˆ)\")\n",
        "                    else:\n",
        "                        print(f\"  [API] ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëª¨ë¸: {model}, Timeout: {timeout}ì´ˆ)...\")\n",
        "\n",
        "                    # API í˜¸ì¶œ (Timeout ì ìš©)\n",
        "                    start_time = time.time()\n",
        "                    try:\n",
        "                        if hasattr(signal, 'SIGALRM'):\n",
        "                            with time_limit(timeout):\n",
        "                                response = client.models.generate_content(\n",
        "                                    model=model,\n",
        "                                    contents=prompt\n",
        "                                )\n",
        "                        else:\n",
        "                            # Windows: ë‹¨ìˆœ í˜¸ì¶œ\n",
        "                            response = client.models.generate_content(\n",
        "                                model=model,\n",
        "                                contents=prompt\n",
        "                            )\n",
        "                            elapsed = time.time() - start_time\n",
        "                            if elapsed > timeout:\n",
        "                                raise TimeoutError(f\"API í˜¸ì¶œì´ {timeout}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\")\n",
        "                    except TimeoutError as e:\n",
        "                        print(f\"  [API] â±ï¸ Timeout ({timeout}ì´ˆ ì´ˆê³¼). ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...\")\n",
        "                        break  # ë‹¤ìŒ ëª¨ë¸ë¡œ\n",
        "\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"  [API] âœ“ ì„±ê³µ! (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)\")\n",
        "                    return response.text.strip()\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_str = str(e)\n",
        "\n",
        "                    # 429 Rate Limit ì—ëŸ¬ ì²˜ë¦¬\n",
        "                    if \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str:\n",
        "                        print(f\"  [API] âš  Rate Limit ë„ë‹¬. ë‹¤ìŒ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì „í™˜...\")\n",
        "                        break  # ëŒ€ê¸°í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ëª¨ë¸ë¡œ\n",
        "\n",
        "                    # ê¸°íƒ€ ì—ëŸ¬\n",
        "                    else:\n",
        "                        print(f\"  [API] âœ— ì—ëŸ¬ ë°œìƒ: {error_str}\")\n",
        "                        if attempt < max_retries - 1:\n",
        "                            continue\n",
        "                        else:\n",
        "                            if model_idx < len(models) - 1:\n",
        "                                print(f\"  [API] ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...\")\n",
        "                                break\n",
        "                            else:\n",
        "                                raise Exception(f\"ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨: {error_str}\")\n",
        "\n",
        "        raise Exception(\"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: ëª¨ë“  ì¬ì‹œë„ ì†Œì§„\")\n",
        "\n",
        "    def split_query(self, text_query: str) -> tuple[list[str], str]:\n",
        "        \"\"\"\n",
        "        Gemini APIë¥¼ í™œìš©í•˜ì—¬ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ë™ì‘ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
        "        API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, í•œêµ­ì–´ ì ‘ì†ì‚¬ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ì„ ì‹œë„í•©ë‹ˆë‹¤.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (ë¶„í• ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ë¶„í•  ì´ìœ  ì„¤ëª…)\n",
        "        \"\"\"\n",
        "        api_start_time = time.time()\n",
        "        print(f\"Thinking with Gemini (Korean Mode)... Query: '{text_query}'\")\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 1. Gemini API í˜¸ì¶œ (Primary Strategy)\n",
        "        # ---------------------------------------------------------\n",
        "        prompt = f\"\"\"\n",
        "        ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ìœ„í•œ ì–¸ì–´ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.\n",
        "        ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´(Query)ê°€ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì—¬ëŸ¬ ë™ì‘(Sequence)ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´, ì´ë¥¼ ë¶„í• í•˜ì„¸ìš”.\n",
        "        ê·¸ í›„, ê·¸ê²ƒë“¤ì„ CLIP ëª¨ë¸ì´ ê°€ì¥ ì˜ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì •êµí•œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­í•œ í›„ JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
        "\n",
        "        [ê·œì¹™]\n",
        "        1. ë¬¸ë§¥ìƒ ì‹œê°„ì˜ íë¦„(ì˜ˆ: ~í•˜ê³  ë‚˜ì„œ, ~í•œ ë’¤ì—, ~í•˜ë‹¤ê°€)ì´ ìˆìœ¼ë©´ ë™ì‘ ë‹¨ìœ„ë¡œ ìª¼ê°œì„¸ìš”.\n",
        "        2. ë‹¨ìˆœí•œ ë¬˜ì‚¬ë‚˜ ë‹¨ì¼ ë™ì‘ì´ë©´ ìš”ì†Œê°€ 1ê°œì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n",
        "        3. 1íšŒ ì´ìƒ ìª¼ê°œì§€ ë§ˆì„¸ìš”. (ìš”ì†ŒëŠ” 2ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.)\n",
        "        4. ë°˜í™˜ê°’ì€ ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ê°ì²´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (Markdown ì œì™¸)\n",
        "        5. ë¶„í• ëœ ë¬¸ì¥ì€ ê²€ìƒ‰ì´ ì˜ ë˜ë„ë¡ ê¸°ë³¸í˜•(ì˜ˆ: 'ë‹¬ë¦¬ê³ ' -> 'ë‹¬ë¦¬ëŠ” ì‚¬ëŒ')ì´ë‚˜ ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.\n",
        "        6. ë¶„í•  ì—¬ë¶€ì™€ ê·¸ ì´ìœ ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.\n",
        "\n",
        "        [ë°˜í™˜ í˜•ì‹]\n",
        "        {{\n",
        "        \"results\": [\n",
        "            {{\"ko\": \"í•œêµ­ì–´ ë™ì‘ 1\", \"en\": \"English description 1\"}},\n",
        "            {{\"ko\": \"í•œêµ­ì–´ ë™ì‘ 2\", \"en\": \"English description 2\"}}\n",
        "        ],\n",
        "        \"reason\": \"ë¶„í•  ë° ë²ˆì—­ ì´ìœ \"\n",
        "        }}\n",
        "\n",
        "        [ì˜ˆì‹œ 1]\n",
        "        Query: \"ê³µì„ ë˜ì§€ê³  ë‚˜ì„œ ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ\"\n",
        "        Output: {{\n",
        "            \"results\": [\n",
        "                {{\"ko\": \"ê³µì„ ë˜ì§€ëŠ” ì‚¬ëŒ\", \"en\": \"Person throwing a ball\"}},\n",
        "                {{\"ko\": \"ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ\", \"en\": \"Person falling down\"}}\n",
        "            ],\n",
        "            \"reason\": \"ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'ë˜ì§€ê³  ë‚˜ì„œ'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ê°œì˜ ì—°ì† ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.\"\n",
        "        }}\n",
        "\n",
        "        [ì˜ˆì‹œ 2]\n",
        "        Query: \"ì›ƒê³  ìˆëŠ” ì•„ê¸°\"\n",
        "        Output: {{\n",
        "            \"results\": [\n",
        "                {{\"ko\": \"ì›ƒê³  ìˆëŠ” ì•„ê¸°\", \"en\": \"Baby laughing\"}}\n",
        "            ],\n",
        "            \"reason\": \"ë‹¨ì¼ ë™ì‘/ìƒíƒœë¥¼ ë¬˜ì‚¬í•˜ëŠ” ì¿¼ë¦¬ë¡œ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "        }}\n",
        "\n",
        "        [ì˜ˆì‹œ 3]\n",
        "        Query: \"ìš”ë¦¬ë¥¼ í•˜ë‹¤ê°€ ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì\"\n",
        "        Output: {{\n",
        "            \"results\": [\n",
        "                {{\"ko\": \"ìš”ë¦¬ë¥¼ í•˜ëŠ” ë‚¨ì\", \"en\": \"Man cooking\"}},\n",
        "                {{\"ko\": \"ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì\", \"en\": \"Man panicking after fire\"}}\n",
        "            ],\n",
        "            \"reason\": \"'í•˜ë‹¤ê°€'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ìˆœì„œìƒ ì„ í–‰ ë™ì‘ê³¼ í›„í–‰ ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.\"\n",
        "        }}\n",
        "\n",
        "        [ì‹¤ì œ ì…ë ¥]\n",
        "        Query: \"{text_query}\"\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            # API í˜¸ì¶œ (Retry ë¡œì§ í¬í•¨)\n",
        "            result_text = self._call_gemini_with_retry(prompt)\n",
        "\n",
        "            if result_text.startswith(\"```\"):\n",
        "                result_text = re.sub(r\"```(json)?\", \"\", result_text).strip()\n",
        "                result_text = re.sub(r\"```\", \"\", result_text).strip()\n",
        "\n",
        "            # JSON íŒŒì‹±\n",
        "            result = json.loads(result_text)\n",
        "\n",
        "            if isinstance(result, dict) and \"results\" in result:\n",
        "                actions = [x[\"en\"] for x in result[\"results\"]]\n",
        "                reason = result.get(\"reason\", \"ë¶„í•  ì™„ë£Œ\")\n",
        "\n",
        "                api_time = time.time() - api_start_time\n",
        "                self.timing_info[\"api_call_time\"] = api_time\n",
        "                print(f\" -> Gemini Split Result (EN): {actions} \\n Reason: {reason}\")\n",
        "                print(f\" -> API í˜¸ì¶œ ì‹œê°„: {api_time:.2f}ì´ˆ\")\n",
        "                return actions, f\"[Gemini API] {reason}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini API Error: {e}. Switching to Fallback.\")\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 2. ê·œì¹™ ê¸°ë°˜ Fallback (í•œêµ­ì–´ ì ‘ì†ì‚¬ ì²˜ë¦¬)\n",
        "        # ---------------------------------------------------------\n",
        "        # APIê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‘ë‹µì´ ì´ìƒí•  ê²½ìš° ì‘ë™í•˜ëŠ” ë¹„ìƒ ë¡œì§ì…ë‹ˆë‹¤.\n",
        "        # í•œêµ­ì–´ì—ì„œ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”í•œ í‘œí˜„ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ìë¦…ë‹ˆë‹¤.\n",
        "\n",
        "        api_time = time.time() - api_start_time\n",
        "        self.timing_info[\"api_call_time\"] = api_time\n",
        "\n",
        "        delimiters = [\n",
        "            \" ê·¸ë¦¬ê³  \", \" ë‹¤ìŒì— \", \" ê·¸ í›„ \", \" ê·¸ ë’¤ì— \", \" ë‚˜ì„œ \",\n",
        "            \" í•˜ë‹¤ê°€ \", \"ë‹¤ê°€ \"\n",
        "        ]\n",
        "\n",
        "        # ê°€ì¥ ë¨¼ì € ë°œê²¬ë˜ëŠ” êµ¬ë¶„ìë¡œ 1íšŒë§Œ ë¶„í•  ì‹œë„ (ë³µì¡ì„± ë°©ì§€)\n",
        "        for delim in delimiters:\n",
        "            if delim in text_query:\n",
        "                parts = text_query.split(delim)\n",
        "                # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
        "                clean_parts = [p.strip() for p in parts if p.strip()]\n",
        "                if len(clean_parts) > 1:\n",
        "                    print(f\" -> Rule-based Split Result: {clean_parts}\")\n",
        "                    print(f\" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ\")\n",
        "                    return clean_parts, f\"[Rule-based] '{delim.strip()}' êµ¬ë¶„ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "        # ë¶„í•  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        print(\"Final split result: \", [text_query])\n",
        "        print(f\" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ\")\n",
        "        return [text_query], \"[Rule-based] ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ì´ ì—†ì–´ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    def calculate_sequential_score(self, frames, sub_queries):\n",
        "        \"\"\"\n",
        "        [ë³€ê³¡ì  íƒì§€ ë¡œì§]\n",
        "        ì¿¼ë¦¬ê°€ A -> Bë¡œ ë‚˜ë‰˜ì—ˆì„ ë•Œ, í”„ë ˆì„ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ìµœì ì˜ ë¶„í•  ì§€ì ì„ ì°¾ì•„\n",
        "        (Aìœ ì‚¬ë„ + Bìœ ì‚¬ë„)ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "        Returns: (max_score, scores_matrix, best_split_index)\n",
        "        \"\"\"\n",
        "        # (q_frames, 2_sub_queries) matrix\n",
        "        scores_matrix = self.mm.get_clip_scores(frames, sub_queries)\n",
        "        q_len = len(frames)\n",
        "        max_score = -1.0\n",
        "        best_split = -1\n",
        "\n",
        "        # Linear Scan to find Change Point\n",
        "        # ìµœì†Œ 20% ì§€ì ë¶€í„° 80% ì§€ì  ì‚¬ì´ì—ì„œ ë¶„í•  ì‹œë„\n",
        "        start_idx = int(q_len * 0.2)\n",
        "        end_idx = int(q_len * 0.8)\n",
        "\n",
        "        if len(sub_queries) == 2:\n",
        "            score_A = scores_matrix[:, 0] # Similarity curve for Query A\n",
        "            score_B = scores_matrix[:, 1] # Similarity curve for Query B\n",
        "\n",
        "            for t in range(start_idx, end_idx):\n",
        "                # t ì‹œì ê¹Œì§€ëŠ” A, t ì´í›„ëŠ” B\n",
        "                avg_A = np.mean(score_A[:t])\n",
        "                avg_B = np.mean(score_B[t:])\n",
        "                combined_score = (avg_A + avg_B) / 2\n",
        "\n",
        "                if combined_score > max_score:\n",
        "                    max_score = combined_score\n",
        "                    best_split = t\n",
        "        else:\n",
        "            max_score = np.mean(np.max(scores_matrix, axis=1))\n",
        "\n",
        "        return float(max_score), scores_matrix, best_split\n",
        "\n",
        "    def normalize_score(self, raw_score: float) -> float:\n",
        "        \"\"\"\n",
        "        CLIP ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ 0-100 ì ìˆ˜ë¡œ ë³€í™˜\n",
        "        - 0.2 ì´í•˜: 0ì \n",
        "        - 0.45 ì´ìƒ: 100ì \n",
        "        \"\"\"\n",
        "        lower_bound = 0.20\n",
        "        upper_bound = 0.45\n",
        "\n",
        "        # ì •ê·œí™” ê³„ì‚°\n",
        "        normalized = (raw_score - lower_bound) / (upper_bound - lower_bound) * 100\n",
        "\n",
        "        # 0~100 ì‚¬ì´ë¡œ í´ë¦¬í•‘\n",
        "        return float(np.clip(normalized, 0, 100))\n",
        "\n",
        "    def search(self, original_query, sub_queries, p_sec, q_frames, k_top, weight_clip = 0.7, weight_semantic = 0.3, enable_visualization=True, save_path=\"results\"):\n",
        "        \"\"\"\n",
        "        Adaptive Search Engine ì‹¤í–‰ ë©”ì¸ ë¡œì§\n",
        "        - 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)\n",
        "        - 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)\n",
        "        - 3. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ ë° ì •ë ¬\n",
        "\n",
        "        Args:\n",
        "            enable_visualization: ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™” ì—¬ë¶€\n",
        "            save_path: ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        search_start_time = time.time()\n",
        "\n",
        "        # íƒ€ì´ë° ì´ˆê¸°í™”\n",
        "        total_frame_extraction_time = 0.0\n",
        "        total_clip_inference_time = 0.0\n",
        "        total_blip_inference_time = 0.0\n",
        "\n",
        "        is_sequential = len(sub_queries) > 1\n",
        "\n",
        "        all_windows = []\n",
        "        step_size = p_sec  # ìœˆë„ìš°ê°€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ìˆ˜ì •\n",
        "        current_time = 0.0\n",
        "\n",
        "        # ì „ì²´ ìœˆë„ìš° ê°œìˆ˜ ê³„ì‚° (ì •í™•í•œ ê³„ì‚°)\n",
        "        total_windows = int(np.ceil(self.vp.duration / step_size))\n",
        "\n",
        "        # ì‹¤ì‹œê°„ ì‹œê°í™” ì´ˆê¸°í™”\n",
        "        visualizer = None\n",
        "        if enable_visualization:\n",
        "            try:\n",
        "                visualizer = RealTimeVisualizer(self.vp.duration, k_top, save_path)\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"[ğŸ“Š ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”] ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ì— í‘œì‹œí•©ë‹ˆë‹¤!\")\n",
        "                print(f\"{'='*60}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"ì‹œê°í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì‹œê°í™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
        "                visualizer = None\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[ê²€ìƒ‰ ì‹œì‘] ì´ {total_windows}ê°œ ìœˆë„ìš° ì²˜ë¦¬ ì˜ˆì • (ìœˆë„ìš° í¬ê¸°: {p_sec}ì´ˆ, í”„ë ˆì„ ìƒ˜í”Œ: {q_frames}ê°œ)\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)\n",
        "        window_idx = 0\n",
        "        current_top_window = None\n",
        "\n",
        "        while current_time < self.vp.duration:\n",
        "            window_idx += 1\n",
        "            end_time = min(current_time + p_sec, self.vp.duration)\n",
        "\n",
        "            print(f\"[Window {window_idx}/{total_windows}] ì²˜ë¦¬ ì¤‘: {self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}\")\n",
        "\n",
        "            # í”„ë ˆì„ ì¶”ì¶œ ì‹œê°„ ì¸¡ì •\n",
        "            frame_start = time.time()\n",
        "            frames = self.vp.extract_window_frames(current_time, end_time, q_frames, window_idx, total_windows)\n",
        "            frame_time = time.time() - frame_start\n",
        "            total_frame_extraction_time += frame_time\n",
        "\n",
        "            # CLIP ì¶”ë¡  ì‹œê°„ ì¸¡ì •\n",
        "            clip_start = time.time()\n",
        "            if is_sequential:\n",
        "                raw_score, scores_matrix, best_split = self.calculate_sequential_score(frames, sub_queries)\n",
        "                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥ (ì‹œí€€ì…œì˜ ê²½ìš° ë‘ ì¿¼ë¦¬ì— ëŒ€í•œ ì ìˆ˜)\n",
        "                frame_scores = {\n",
        "                    f\"query_{i}\": scores_matrix[:, i].tolist()\n",
        "                    for i in range(len(sub_queries))\n",
        "                }\n",
        "                frame_scores[\"best_split_index\"] = int(best_split) if best_split != -1 else None\n",
        "            else:\n",
        "                raw_scores_matrix = self.mm.get_clip_scores(frames, sub_queries)\n",
        "                raw_score = float(np.mean(raw_scores_matrix))\n",
        "                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥\n",
        "                frame_scores = {\n",
        "                    f\"query_{i}\": raw_scores_matrix[:, i].tolist()\n",
        "                    for i in range(len(sub_queries))\n",
        "                }\n",
        "            clip_time = time.time() - clip_start\n",
        "            total_clip_inference_time += clip_time\n",
        "\n",
        "            clip_score_norm = self.normalize_score(raw_score)\n",
        "            print(f\"  -> ì •ê·œí™” CLIP ì ìˆ˜: {clip_score_norm:.2f} (í”„ë ˆì„ ì¶”ì¶œ: {frame_time:.2f}ì´ˆ, CLIP ì¶”ë¡ : {clip_time:.2f}ì´ˆ)\")\n",
        "\n",
        "            window_data = {\n",
        "                \"start\": current_time,\n",
        "                \"end\": end_time,\n",
        "                \"timestamp\": f\"{self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}\",\n",
        "                \"raw_score\": raw_score,           # ì°¸ê³ ìš© ì›ë³¸ ì ìˆ˜\n",
        "                \"clip_score_norm\": clip_score_norm,    # ì •ê·œí™”ëœ ì ìˆ˜ (JSON ì €ì¥ìš©)\n",
        "                \"frame_scores\": frame_scores,  # í”„ë ˆì„ë³„ ì ìˆ˜ ì¶”ê°€\n",
        "                \"mid_frame\": frames[len(frames)//2] # ë³´ì •ì„ ìœ„í•´ ì¤‘ê°„ í”„ë ˆì„ ì €ì¥\n",
        "            }\n",
        "            all_windows.append(window_data)\n",
        "\n",
        "            # í˜„ì¬ê¹Œì§€ ìµœê³  ì ìˆ˜ ìœˆë„ìš° ì¶”ì \n",
        "            if current_top_window is None or clip_score_norm > current_top_window['clip_score_norm']:\n",
        "                current_top_window = window_data\n",
        "                print(f\"  â­ ìƒˆë¡œìš´ Top ìœˆë„ìš° ë°œê²¬! ({current_top_window['timestamp']})\\n\")\n",
        "            else:\n",
        "                print(f\"  [í˜„ì¬ Top] {current_top_window['timestamp']} (ì ìˆ˜: {current_top_window['clip_score_norm']:.4f})\\n\")\n",
        "\n",
        "            # ì‹¤ì‹œê°„ ì‹œê°í™” ì—…ë°ì´íŠ¸\n",
        "            if visualizer:\n",
        "                try:\n",
        "                    visualizer.update({\n",
        "                        'start': current_time,\n",
        "                        'end': end_time,\n",
        "                        'clip_score_norm': clip_score_norm\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"  [ì‹œê°í™” ê²½ê³ ] ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "            current_time += step_size\n",
        "\n",
        "        # CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ Kê°œ ì„ ë³„\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[1ì°¨ ê²€ìƒ‰ ì™„ë£Œ] CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ {k_top}ê°œ í›„ë³´ ì„ ë³„\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        all_windows.sort(key=lambda x: x[\"clip_score_norm\"], reverse=True)\n",
        "        top_k_candidates = all_windows[:k_top]\n",
        "\n",
        "        for idx, item in enumerate(top_k_candidates, 1):\n",
        "            print(f\"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}\")\n",
        "\n",
        "        # 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)\n",
        "        if self.mm.use_blip:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[2ì°¨ ë³´ì • ì‹œì‘] BLIP-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ {k_top}ê°œ í›„ë³´ ë³´ì • ì¤‘...\")\n",
        "            print(f\"{'='*60}\\n\")\n",
        "\n",
        "            for idx, item in enumerate(top_k_candidates, 1):\n",
        "                print(f\"[í›„ë³´ {idx}/{k_top}] {item['timestamp']}\")\n",
        "\n",
        "                # A. BLIP-2ë¡œ í”„ë ˆì„ ì„¤ëª…(Caption) ìƒì„± - ì‹œê°„ ì¸¡ì •\n",
        "                blip_start = time.time()\n",
        "                generated_caption = self.mm.generate_caption(item['mid_frame'])\n",
        "                blip_time = time.time() - blip_start\n",
        "                total_blip_inference_time += blip_time\n",
        "\n",
        "                item['blip_caption'] = generated_caption\n",
        "\n",
        "                # B. ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ìƒì„±ëœ ìº¡ì…˜ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (Text-to-Text)\n",
        "                semantic_start = time.time()\n",
        "                semantic_sim = self.mm.compute_text_similarity(original_query, generated_caption)\n",
        "                semantic_time = time.time() - semantic_start\n",
        "                total_blip_inference_time += semantic_time\n",
        "\n",
        "                item['semantic_consistency'] = semantic_sim\n",
        "\n",
        "                # C. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ (ì•™ìƒë¸”)\n",
        "                item['final_score'] = (item['clip_score_norm'] * weight_clip) + (semantic_sim * weight_semantic)\n",
        "\n",
        "                print(f\"  -> ìƒì„±ëœ ìº¡ì…˜: {generated_caption}\")\n",
        "                print(f\"  -> ì˜ë¯¸ ìœ ì‚¬ë„: {semantic_sim:.4f}\")\n",
        "                print(f\"  -> ìµœì¢… ì ìˆ˜: {item['final_score']:.4f}\")\n",
        "                print(f\"  -> BLIP-2 ì²˜ë¦¬ ì‹œê°„: {blip_time + semantic_time:.2f}ì´ˆ\\n\")\n",
        "\n",
        "            # ë³´ì •ëœ ìµœì¢… ì ìˆ˜ë¡œ ë‹¤ì‹œ ì •ë ¬\n",
        "            top_k_candidates.sort(key=lambda x: x.get('final_score', x['clip_score_norm']), reverse=True)\n",
        "\n",
        "            print(f\"{'='*60}\")\n",
        "            print(f\"[ìµœì¢… ìˆœìœ„]\")\n",
        "            print(f\"{'='*60}\")\n",
        "            for idx, item in enumerate(top_k_candidates, 1):\n",
        "                print(f\"{idx}. {item['timestamp']} - ìµœì¢… ì ìˆ˜: {item.get('final_score', item['clip_score_norm']):.4f}\")\n",
        "        else:\n",
        "            # BLIP-2 ì—†ì„ ë•Œë„ ìµœì¢… ìˆœìœ„ ì¶œë ¥\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"[ìµœì¢… ìˆœìœ„]\")\n",
        "            print(f\"{'='*60}\")\n",
        "            for idx, item in enumerate(top_k_candidates, 1):\n",
        "                print(f\"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # ì‹¤ì‹œê°„ ì‹œê°í™” ìµœì¢… ì—…ë°ì´íŠ¸ (BLIP-2 ë³´ì • ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ì—¬ê¸°ì„œ í˜¸ì¶œ)\n",
        "        if visualizer:\n",
        "            try:\n",
        "                visualizer.finalize(top_k_candidates)\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"[ğŸ“Š ì‹œê°í™” ì™„ë£Œ] ìµœì¢… Top-{k_top} ê²°ê³¼ê°€ ë¹¨ê°„ìƒ‰ ë³„(â˜…)ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "                print(f\"{'='*60}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ì‹œê°í™” ì˜¤ë¥˜] ìµœì¢… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥ ì „ ì´ë¯¸ì§€ ê°ì²´ ì‚­ì œ (ë©”ëª¨ë¦¬ í™•ë³´)\n",
        "        for item in top_k_candidates:\n",
        "            if 'mid_frame' in item: del item['mid_frame']\n",
        "\n",
        "        # all_windowsì—ì„œë„ ì´ë¯¸ì§€ ê°ì²´ ì‚­ì œ\n",
        "        for item in all_windows:\n",
        "            if 'mid_frame' in item: del item['mid_frame']\n",
        "\n",
        "        # íƒ€ì´ë° ì •ë³´ ì €ì¥\n",
        "        total_search_time = time.time() - search_start_time\n",
        "        self.timing_info[\"frame_extraction_time\"] = total_frame_extraction_time\n",
        "        self.timing_info[\"clip_inference_time\"] = total_clip_inference_time\n",
        "        self.timing_info[\"blip_inference_time\"] = total_blip_inference_time\n",
        "        self.timing_info[\"total_search_time\"] = total_search_time\n",
        "\n",
        "        # visualizer ê°ì²´ë¥¼ ë°˜í™˜ (mainì—ì„œ ì €ì¥)\n",
        "        return top_k_candidates, all_windows, visualizer\n",
        "\n",
        "print(\"âœ… AdaptiveSearchEngine í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89a4c0e",
      "metadata": {
        "id": "b89a4c0e"
      },
      "source": [
        "## 6. Real-time Visualization\n",
        "\n",
        "ê²€ìƒ‰ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ì— í‘œì‹œí•˜ê³  ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9878383d",
      "metadata": {
        "id": "9878383d"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. Real-time Visualization\n",
        "# ==========================================\n",
        "class RealTimeVisualizer:\n",
        "    def __init__(self, total_duration, k_top, save_path=\"results\"):\n",
        "        \"\"\"\n",
        "        ì‹¤ì‹œê°„ ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            total_duration: ë¹„ë””ì˜¤ ì´ ê¸¸ì´ (ì´ˆ)\n",
        "            k_top: Top-K ê°œìˆ˜\n",
        "            save_path: ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ\n",
        "        \"\"\"\n",
        "        self.total_duration = total_duration\n",
        "        self.k_top = k_top\n",
        "        self.save_path = save_path\n",
        "        self.window_data = []\n",
        "        self.current_top_k = []\n",
        "        self.is_complete = False\n",
        "        self.save_filename = None\n",
        "\n",
        "        # ê·¸ë˜í”„ ì„¤ì •\n",
        "        plt.ion()  # Interactive mode\n",
        "        self.fig, self.ax = plt.subplots(figsize=(14, 6))\n",
        "        self.fig.suptitle('Real-time Video Search Similarity Scores', fontsize=14, fontweight='bold')\n",
        "\n",
        "    def update(self, window_info):\n",
        "        \"\"\"\n",
        "        ìƒˆë¡œìš´ ìœˆë„ìš° ì •ë³´ë¡œ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸\n",
        "\n",
        "        Args:\n",
        "            window_info: {'start': float, 'end': float, 'clip_score_norm': float, 'is_top_k': bool}\n",
        "        \"\"\"\n",
        "        self.window_data.append(window_info)\n",
        "\n",
        "        # í˜„ì¬ê¹Œì§€ì˜ Top-K ê³„ì‚°\n",
        "        sorted_windows = sorted(self.window_data, key=lambda x: x['clip_score_norm'], reverse=True)\n",
        "        self.current_top_k = sorted_windows[:self.k_top]\n",
        "\n",
        "        self._draw()\n",
        "\n",
        "    def finalize(self, final_top_k):\n",
        "        \"\"\"\n",
        "        ê²€ìƒ‰ ì™„ë£Œ í›„ ìµœì¢… Top-K í‘œì‹œ\n",
        "\n",
        "        Args:\n",
        "            final_top_k: ìµœì¢… Top-K ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸\n",
        "        \"\"\"\n",
        "        self.is_complete = True\n",
        "        self.final_top_k = final_top_k\n",
        "        self._draw()\n",
        "\n",
        "    def _draw(self):\n",
        "        \"\"\"ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\"\"\"\n",
        "        self.ax.clear()\n",
        "\n",
        "        if not self.window_data:\n",
        "            return\n",
        "\n",
        "        # ì‹œê°„ì¶•ê³¼ ì ìˆ˜ ë°ì´í„° ì¤€ë¹„\n",
        "        times = [(w['start'] + w['end']) / 2 for w in self.window_data]\n",
        "        scores = [w['clip_score_norm'] for w in self.window_data]\n",
        "\n",
        "        # 1. ê¸°ë³¸ ì ìˆ˜ ì„  ê·¸ë˜í”„ (íšŒìƒ‰)\n",
        "        self.ax.plot(times, scores, color='#CCCCCC', linewidth=1, alpha=0.6, zorder=1)\n",
        "\n",
        "        # 2. ëª¨ë“  ìœˆë„ìš° ì  (ì‘ì€ íŒŒë€ìƒ‰)\n",
        "        self.ax.scatter(times, scores, color='#4A90E2', s=30, alpha=0.5, zorder=2)\n",
        "\n",
        "        # 3. í˜„ì¬ Top-K í›„ë³´ (ë…¸ë€ìƒ‰ í° ì )\n",
        "        if not self.is_complete:\n",
        "            top_k_times = [(w['start'] + w['end']) / 2 for w in self.current_top_k]\n",
        "            top_k_scores = [w['clip_score_norm'] for w in self.current_top_k]\n",
        "            self.ax.scatter(top_k_times, top_k_scores, color='#FFD700', s=200,\n",
        "                          edgecolors='#FFA500', linewidths=2, zorder=4,\n",
        "                          label=f'Current Top-{self.k_top}', marker='o', alpha=0.9)\n",
        "\n",
        "            # ë°˜ì§ì´ëŠ” íš¨ê³¼ë¥¼ ìœ„í•œ ì™¸ê³½ì„ \n",
        "            for t, s in zip(top_k_times, top_k_scores):\n",
        "                circle = Circle((t, s), radius=0.3, color='#FFD700', alpha=0.3, zorder=3)\n",
        "                self.ax.add_patch(circle)\n",
        "\n",
        "        # 4. ìµœì¢… Top-K (ë¹¨ê°„ìƒ‰ í° ì )\n",
        "        if self.is_complete:\n",
        "            final_times = [(w['start'] + w['end']) / 2 for w in self.final_top_k]\n",
        "            final_scores = [w['clip_score_norm'] for w in self.final_top_k]\n",
        "            self.ax.scatter(final_times, final_scores, color='#E74C3C', s=250,\n",
        "                          edgecolors='#C0392B', linewidths=3, zorder=5,\n",
        "                          label=f'Final Top-{self.k_top}', marker='*', alpha=1.0)\n",
        "\n",
        "            # ìˆœìœ„ í‘œì‹œ\n",
        "            for idx, (t, s, w) in enumerate(zip(final_times, final_scores, self.final_top_k), 1):\n",
        "                self.ax.annotate(f'#{idx}', xy=(t, s), xytext=(5, 5),\n",
        "                               textcoords='offset points', fontsize=10,\n",
        "                               fontweight='bold', color='#E74C3C',\n",
        "                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='#E74C3C', alpha=0.8))\n",
        "\n",
        "        # ê·¸ë˜í”„ ì„¤ì •\n",
        "        self.ax.set_xlabel('Video Time (seconds)', fontsize=11, fontweight='bold')\n",
        "        self.ax.set_ylabel('Normalized Similarity Score', fontsize=11, fontweight='bold')\n",
        "        self.ax.set_xlim(0, self.total_duration)\n",
        "        self.ax.set_ylim(0, 105)\n",
        "        self.ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        self.ax.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "        # ì§„í–‰ë¥  í‘œì‹œ\n",
        "        if self.window_data:\n",
        "            progress = (self.window_data[-1]['end'] / self.total_duration) * 100\n",
        "            status = \"COMPLETE âœ“\" if self.is_complete else f\"Processing... {progress:.1f}%\"\n",
        "            self.ax.text(0.02, 0.98, status, transform=self.ax.transAxes,\n",
        "                       fontsize=11, fontweight='bold', verticalalignment='top',\n",
        "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.pause(0.01)\n",
        "\n",
        "    def save_and_close(self, filename_base):\n",
        "        \"\"\"ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ê³  ì°½ ë‹«ê¸°\"\"\"\n",
        "        plt.ioff()\n",
        "\n",
        "        # íŒŒì¼ëª… ìƒì„±\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.save_filename = f\"viz_{filename_base}.png\"\n",
        "        save_path = os.path.join(self.save_path, self.save_filename)\n",
        "\n",
        "        # ì´ë¯¸ì§€ë¡œ ì €ì¥\n",
        "        self.fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"  [ì‹œê°í™”] ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {self.save_filename}\")\n",
        "\n",
        "        # ì°½ ë‹«ê¸°\n",
        "        plt.close(self.fig)\n",
        "\n",
        "        return self.save_filename\n",
        "\n",
        "print(\"âœ… RealTimeVisualizer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cf0779",
      "metadata": {
        "id": "d3cf0779"
      },
      "source": [
        "## 7. ë©”ì¸ í•¨ìˆ˜ & ì‹¤í–‰\n",
        "\n",
        "ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ë¹„ë””ì˜¤ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì‹¤í–‰ ë°©ë²•\n",
        "```python\n",
        "# ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ê±°ë‚˜, main() í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì„¸ìš”\n",
        "main()\n",
        "```\n",
        "\n",
        "### íŒŒë¼ë¯¸í„° ì¡°ì •\n",
        "- `VIDEO_PATH`: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
        "- `QUERY`: ê²€ìƒ‰ ì¿¼ë¦¬ (í•œêµ­ì–´)\n",
        "- `p_list`: ìœˆë„ìš° í¬ê¸° (ì´ˆ)\n",
        "- `q_list`: ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜\n",
        "- `k_list`: Top-K ê°œìˆ˜\n",
        "- `USE_BLIP`: BLIP-2 ì‚¬ìš© ì—¬ë¶€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d5cb64",
      "metadata": {
        "id": "65d5cb64"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 5. Main Execution\n",
        "# ==========================================\n",
        "def main():\n",
        "    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘\n",
        "    program_start_time = time.time()\n",
        "\n",
        "    # --- Configurations ---\n",
        "    VIDEO_PATH = \"sample_video.mp4\" # ì¤€ë¹„ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
        "    SAVE_PATH = \"results\"\n",
        "    if not os.path.exists(SAVE_PATH):\n",
        "        os.makedirs(SAVE_PATH)\n",
        "    else:\n",
        "        print(f\"Save path '{SAVE_PATH}' already exists. Results will be saved here.\")\n",
        "    QUERY = \"ë°”ë‹¥ì— ë–¨ì–´ì§„ ì‹ ìš©ì¹´ë“œ\"\n",
        "    # \"ë°”ë‹¥ì— ë–¨ì–´ì§€ëŠ” ì¹´ë“œë¥¼ ë³´ê³  ë‚œê°í•œ í‘œì •ì„ ì§“ëŠ” ë‚¨ì\" # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
        "\n",
        "    # Experiments Parameters\n",
        "    p_list = [2.0, 4.0]      # ìœˆë„ìš° í¬ê¸° (ì´ˆ)\n",
        "    q_list = [12, 24, 48]         # ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜\n",
        "    k_list = [3, 5]          # Top-K ê°œìˆ˜\n",
        "    USE_BLIP = False         # BLIP-2 ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì£¼ì˜)\n",
        "    WEIGHT_CLIP = 0.7\n",
        "    WEIGHT_SEMANTIC = 0.3\n",
        "    USE_LOOP = False         # ë°˜ë³µ ì‹¤í–‰ ì—¬ë¶€\n",
        "\n",
        "    # Initialize\n",
        "    if not os.path.exists(VIDEO_PATH):\n",
        "        print(f\"Error: Video file '{VIDEO_PATH}' not found. Please place a dummy video.\")\n",
        "        return\n",
        "\n",
        "    # ì´ˆê¸°í™” ì‹œê°„ ì¸¡ì •\n",
        "    init_start_time = time.time()\n",
        "    model_manager = ModelManager(use_blip=USE_BLIP)\n",
        "    video_processor = VideoProcessor(VIDEO_PATH)\n",
        "    engine = AdaptiveSearchEngine(model_manager, video_processor)\n",
        "    total_init_time = time.time() - init_start_time\n",
        "\n",
        "    print(f\"\\n[ì¿¼ë¦¬ ë¶„ì„] '{QUERY}'\")\n",
        "    sub_queries, split_reason = engine.split_query(QUERY)\n",
        "    print(f\"[ë¶„í• ëœ ì¿¼ë¦¬] {sub_queries}\\n\")\n",
        "\n",
        "    # Experiment Loop\n",
        "    # ë°˜ë³µ ì‹¤í–‰ í•  ë•Œ\n",
        "    if USE_LOOP:\n",
        "        for p in p_list:\n",
        "            for q in q_list:\n",
        "                for k in k_list:\n",
        "                    print(f\"\\n--- Running Experiment: p={p}, q={q}, k={k} ---\")\n",
        "\n",
        "                    # Perform Search (ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”)\n",
        "                    results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p, q, k, WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)\n",
        "\n",
        "                    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
        "                    total_elapsed_time = time.time() - program_start_time\n",
        "\n",
        "                    # Construct Filename\n",
        "                    model_name = \"CB\" if USE_BLIP else \"Clip\"\n",
        "                    timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    # ìœ ì˜ë¯¸í•œ ê²°ê³¼ ë‚˜ì™”ìœ¼ë©´ _test.json ëŒ€ì‹  .json í™•ì¥ì ì‚¬ìš©\n",
        "                    filename = f\"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json\"\n",
        "                    filename_base = f\"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test\"\n",
        "\n",
        "                    # ì‹œê°í™” ì €ì¥\n",
        "                    if visualizer:\n",
        "                        try:\n",
        "                            viz_filename = visualizer.save_and_close(filename_base)\n",
        "                        except Exception as e:\n",
        "                            print(f\"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "                            viz_filename = None\n",
        "                    else:\n",
        "                        viz_filename = None\n",
        "\n",
        "                    # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘\n",
        "                    timing_data = {\n",
        "                        \"total_time\": round(total_elapsed_time, 2),\n",
        "                        \"init_time\": round(total_init_time, 2),\n",
        "                        \"model_manager_init_time\": round(model_manager.init_time, 2),\n",
        "                        \"clip_load_time\": round(model_manager.clip_load_time, 2),\n",
        "                        \"blip_load_time\": round(model_manager.blip_load_time, 2),\n",
        "                        \"video_processor_init_time\": round(video_processor.init_time, 2),\n",
        "                        \"api_call_time\": round(engine.timing_info[\"api_call_time\"], 2),\n",
        "                        \"frame_extraction_time\": round(engine.timing_info[\"frame_extraction_time\"], 2),\n",
        "                        \"clip_inference_time\": round(engine.timing_info[\"clip_inference_time\"], 2),\n",
        "                        \"blip_inference_time\": round(engine.timing_info[\"blip_inference_time\"], 2),\n",
        "                        \"total_search_time\": round(engine.timing_info[\"total_search_time\"], 2)\n",
        "                    }\n",
        "\n",
        "                    # Output Data Structure\n",
        "                    output_data = {\n",
        "                        \"meta\": {\n",
        "                            \"video_path\": VIDEO_PATH,\n",
        "                            \"query\": QUERY,\n",
        "                            \"sub_queries\": sub_queries,\n",
        "                            \"split_reason\": split_reason,\n",
        "                            \"parameters\": {\"p\": p, \"q\": q, \"k\": k, \"USE_BLIP\": USE_BLIP, \"WEIGHT_CLIP\": WEIGHT_CLIP, \"WEIGHT_SEMANTIC\": WEIGHT_SEMANTIC},\n",
        "                            \"model\": model_name,\n",
        "                            \"timestamp\": timestamp_str\n",
        "                        },\n",
        "                        \"time_used\": timing_data,\n",
        "                        \"results\": results\n",
        "                    }\n",
        "\n",
        "                    # Save to JSON\n",
        "                    with open(os.path.join(SAVE_PATH, filename), \"w\", encoding='utf-8') as f:\n",
        "                        json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "                    # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥\n",
        "                    whole_score_filename = f\"whole_score_{filename}\"\n",
        "                    whole_score_data = {\n",
        "                        \"meta\": {\n",
        "                            \"video_path\": VIDEO_PATH,\n",
        "                            \"query\": QUERY,\n",
        "                            \"sub_queries\": sub_queries,\n",
        "                            \"parameters\": {\"p\": p, \"q\": q, \"k\": k, \"USE_BLIP\": USE_BLIP, \"WEIGHT_CLIP\": WEIGHT_CLIP, \"WEIGHT_SEMANTIC\": WEIGHT_SEMANTIC},\n",
        "                            \"total_windows\": len(all_windows_data),\n",
        "                            \"timestamp\": timestamp_str\n",
        "                        },\n",
        "                        \"all_windows\": all_windows_data\n",
        "                    }\n",
        "                    with open(os.path.join(SAVE_PATH, whole_score_filename), \"w\", encoding='utf-8') as f:\n",
        "                        json.dump(whole_score_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "                    print(f\"\\n[ì €ì¥ ì™„ë£Œ] {filename}\")\n",
        "                    print(f\"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}\")\n",
        "                    print(f\"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨\")\n",
        "                    if viz_filename:\n",
        "                        print(f\"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}\")\n",
        "                    print(f\"[ì´ ì‹¤í–‰ ì‹œê°„] {total_elapsed_time:.2f}ì´ˆ\\n\")\n",
        "\n",
        "    # ë°˜ë³µ ì‹¤í–‰ ì•„ë‹ ë•Œ\n",
        "    else:\n",
        "        results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p_list[0], q_list[0], k_list[0], WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)\n",
        "\n",
        "        # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°\n",
        "        total_elapsed_time = time.time() - program_start_time\n",
        "\n",
        "        model_name = \"CB\" if USE_BLIP else \"Clip\"\n",
        "        timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json\"\n",
        "        filename_base = f\"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test\"\n",
        "\n",
        "        # ì‹œê°í™” ì €ì¥\n",
        "        if visualizer:\n",
        "            try:\n",
        "                viz_filename = visualizer.save_and_close(filename_base)\n",
        "            except Exception as e:\n",
        "                print(f\"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "                viz_filename = None\n",
        "        else:\n",
        "            viz_filename = None\n",
        "\n",
        "        # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘\n",
        "        timing_data = {\n",
        "            \"total_time\": round(total_elapsed_time, 2),\n",
        "            \"init_time\": round(total_init_time, 2),\n",
        "            \"model_manager_init_time\": round(model_manager.init_time, 2),\n",
        "            \"clip_load_time\": round(model_manager.clip_load_time, 2),\n",
        "            \"blip_load_time\": round(model_manager.blip_load_time, 2),\n",
        "            \"video_processor_init_time\": round(video_processor.init_time, 2),\n",
        "            \"api_call_time\": round(engine.timing_info[\"api_call_time\"], 2),\n",
        "            \"frame_extraction_time\": round(engine.timing_info[\"frame_extraction_time\"], 2),\n",
        "            \"clip_inference_time\": round(engine.timing_info[\"clip_inference_time\"], 2),\n",
        "            \"blip_inference_time\": round(engine.timing_info[\"blip_inference_time\"], 2),\n",
        "            \"total_search_time\": round(engine.timing_info[\"total_search_time\"], 2)\n",
        "        }\n",
        "\n",
        "        # Output Data Structure\n",
        "        output_data = {\n",
        "            \"meta\": {\n",
        "                \"video_path\": VIDEO_PATH,\n",
        "                \"query\": QUERY,\n",
        "                \"sub_queries\": sub_queries,\n",
        "                \"split_reason\": split_reason,\n",
        "                \"parameters\": {\"p\": p_list[0], \"q\": q_list[0], \"k\": k_list[0], \"USE_BLIP\": USE_BLIP, \"WEIGHT_CLIP\": WEIGHT_CLIP, \"WEIGHT_SEMANTIC\": WEIGHT_SEMANTIC},\n",
        "                \"model\": model_name,\n",
        "                \"timestamp\": timestamp_str\n",
        "            },\n",
        "            \"time_used\": timing_data,\n",
        "            \"results\": results\n",
        "        }\n",
        "\n",
        "        # Save to JSON\n",
        "        with open(os.path.join(SAVE_PATH, filename), \"w\", encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥\n",
        "        whole_score_filename = f\"whole_score_{filename}\"\n",
        "        whole_score_data = {\n",
        "            \"meta\": {\n",
        "                \"video_path\": VIDEO_PATH,\n",
        "                \"query\": QUERY,\n",
        "                \"sub_queries\": sub_queries,\n",
        "                \"parameters\": {\"p\": p_list[0], \"q\": q_list[0], \"k\": k_list[0], \"USE_BLIP\": USE_BLIP, \"WEIGHT_CLIP\": WEIGHT_CLIP, \"WEIGHT_SEMANTIC\": WEIGHT_SEMANTIC},\n",
        "                \"total_windows\": len(all_windows_data),\n",
        "                \"timestamp\": timestamp_str\n",
        "            },\n",
        "            \"all_windows\": all_windows_data\n",
        "        }\n",
        "        with open(os.path.join(SAVE_PATH, whole_score_filename), \"w\", encoding='utf-8') as f:\n",
        "            json.dump(whole_score_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"[ê²€ìƒ‰ ì™„ë£Œ] ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        print(f\"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}\")\n",
        "        print(f\"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨\")\n",
        "        if viz_filename:\n",
        "            print(f\"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"\\nğŸ“Š [ì „ì²´ ì‹¤í–‰ ì‹œê°„ ë¶„ì„]\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ\")\n",
        "        print(f\"\\n  ğŸ”§ ì´ˆê¸°í™” ë‹¨ê³„:\")\n",
        "        print(f\"     - ModelManager ì´ˆê¸°í™”: {model_manager.init_time:.2f}ì´ˆ\")\n",
        "        print(f\"       â”œâ”€ CLIP ë¡œë“œ: {model_manager.clip_load_time:.2f}ì´ˆ\")\n",
        "        print(f\"       â””â”€ BLIP-2 ë¡œë“œ: {model_manager.blip_load_time:.2f}ì´ˆ\")\n",
        "        print(f\"     - VideoProcessor ì´ˆê¸°í™”: {video_processor.init_time:.2f}ì´ˆ\")\n",
        "        print(f\"     - ì „ì²´ ì´ˆê¸°í™”: {total_init_time:.2f}ì´ˆ\")\n",
        "        print(f\"\\n  ğŸ” ê²€ìƒ‰ ë‹¨ê³„:\")\n",
        "        print(f\"     - API í˜¸ì¶œ (ì¿¼ë¦¬ ë¶„ì„): {engine.timing_info['api_call_time']:.2f}ì´ˆ\")\n",
        "        print(f\"     - í”„ë ˆì„ ì¶”ì¶œ: {engine.timing_info['frame_extraction_time']:.2f}ì´ˆ\")\n",
        "        print(f\"     - CLIP ì¶”ë¡ : {engine.timing_info['clip_inference_time']:.2f}ì´ˆ\")\n",
        "        if USE_BLIP:\n",
        "            print(f\"     - BLIP-2 ì¶”ë¡ : {engine.timing_info['blip_inference_time']:.2f}ì´ˆ\")\n",
        "        print(f\"     - ì „ì²´ ê²€ìƒ‰: {engine.timing_info['total_search_time']:.2f}ì´ˆ\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5051067",
      "metadata": {
        "id": "f5051067"
      },
      "source": [
        "## 8. ì‹¤í–‰ ì˜ˆì œ\n",
        "\n",
        "ìœ„ì˜ main() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1d12c2",
      "metadata": {
        "id": "2e1d12c2"
      },
      "outputs": [],
      "source": [
        "# ì£¼ì„ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”\n",
        "# main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}