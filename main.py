# pip install torch transformers opencv-python pillow numpy google-genai python-dotenv matplotlib
# í˜¹ì‹œ íŒŒì´í† ì¹˜ ë³´ì•ˆ ê´€ë ¨ ì—ëŸ¬ ëœ¨ë©´... ì´ê±° í•´ì£¼ë©´ ë¨ 
# pip install --upgrade torch torchvision torchaudio
import os
import cv2
import torch
import json
import datetime
import numpy as np
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, Blip2Processor, Blip2ForConditionalGeneration
from typing import List, Dict, Union, Tuple
from google import genai
import json
import re
from dotenv import load_dotenv
import time
import random
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Circle

# ==========================================
# Gemini API ì„¤ì • (AdaptiveSearchEngine ë‚´ë¶€ í˜¹ì€ ì™¸ë¶€ì— ì„ ì–¸)
# ==========================================
# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=GEMINI_API_KEY, http_options=genai.types.HttpOptions(api_version="v1"))
if client is not None:
    print("Gemini Client initialized successfully")
else:
    print("Gemini Client initialization failed")
    exit()
# # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
# for model in client.models.list():
#     print(f"Model Name: {model.name}, Supported Methods: {model.supported_actions}")
# exit()
    
# ==========================================
# 1. Model Manager (CLIP & BLIP-2)
# ==========================================
class ModelManager:
    def __init__(self, use_blip=False, device=None):
        start_time = time.time()
        print("Initializing ModelManager...")
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading models on {self.device}...")

        # Load CLIP (Base Model)
        clip_start = time.time()
        self.clip_processor = CLIPProcessor.from_pretrained(
            "openai/clip-vit-base-patch32", 
            use_fast=True  # ì´ ì˜µì…˜ì„ ì¶”ê°€í•˜ë©´ Rust ê¸°ë°˜ì˜ ë¹ ë¥¸ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        )
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
        self.clip_model.eval()
        clip_time = time.time() - clip_start
        print(f"CLIP Model loaded ({clip_time:.2f}ì´ˆ)")

        # Load BLIP-2 (Refinement Model) - Optional
        self.use_blip = use_blip
        self.blip_processor = None
        self.blip_model = None
        blip_time = 0.0
        
        if self.use_blip:
            print("Loading BLIP-2 (this might take memory)...")
            blip_start = time.time()
            self.blip_processor = Blip2Processor.from_pretrained(
                "Salesforce/blip2-opt-2.7b",
                use_fast=True)
            self.blip_model = Blip2ForConditionalGeneration.from_pretrained(
                "Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16
            ).to(self.device)
            self.blip_model.eval()
            blip_time = time.time() - blip_start
            print(f"BLIP-2 Model loaded ({blip_time:.2f}ì´ˆ)")
        
        self.init_time = time.time() - start_time
        self.clip_load_time = clip_time
        self.blip_load_time = blip_time
        print(f"ModelManager ì´ˆê¸°í™” ì™„ë£Œ (ì´ {self.init_time:.2f}ì´ˆ)")
            

    def get_clip_scores(self, images: List[Image.Image], text_queries: List[str]) -> np.ndarray:
        """
        Computes cosine similarity matrix between images and texts.
        Returns: (n_images, n_queries) numpy array
        """
        inputs = self.clip_processor(text=text_queries, images=images, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            outputs = self.clip_model(**inputs)
            # íŠ¹ì§• ë²¡í„°(Embedding)ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì„œ ì •ê·œí™” í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            image_embeds = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)
            text_embeds = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)
            cosine_sim = torch.matmul(image_embeds, text_embeds.T)
        return cosine_sim.cpu().numpy()

    def generate_caption(self, image: Image.Image) -> str:
        """Generates caption using BLIP-2"""
        if not self.use_blip:
            return ""
        inputs = self.blip_processor(images=image, return_tensors="pt").to(self.device, torch.float16)
        generated_ids = self.blip_model.generate(**inputs)
        return self.blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    
    def get_text_features(self, text_list: List[str]):
        """í…ìŠ¤íŠ¸ë¥¼ CLIP ë²¡í„°ë¡œ ë³€í™˜ (í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ë¹„êµìš©)"""
        inputs = self.clip_processor(text=text_list, return_tensors="pt", padding=True).to(self.device)
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**inputs)
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def compute_text_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°"""
        feat1 = self.get_text_features([text1])
        feat2 = self.get_text_features([text2])
        sim = torch.matmul(feat1, feat2.T)
        return float(sim.cpu().numpy())

# ==========================================
# 2. Video Processor
# ==========================================
class VideoProcessor:
    def __init__(self, video_path):
        start_time = time.time()
        self.video_path = video_path
        self.cap = cv2.VideoCapture(video_path)
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.duration = self.total_frames / self.fps
        self.init_time = time.time() - start_time
        print(f"Video processor initialized ({self.init_time:.2f}ì´ˆ)")
    def extract_window_frames(self, start_sec, end_sec, num_samples_q, window_idx=None, total_windows=None) -> List[Image.Image]:
        """
        Extracts 'q' frames uniformly from the window [start_sec, end_sec].
        """
        frames = []
        start_frame = int(start_sec * self.fps)
        end_frame = int(end_sec * self.fps)
        
        # Uniform sampling indices
        indices = np.linspace(start_frame, end_frame - 1, num_samples_q, dtype=int)
        
        for frame_num, idx in enumerate(indices, 1):
            if window_idx is not None and total_windows is not None:
                print(f"  [Window {window_idx}/{total_windows}] í”„ë ˆì„ ì¶”ì¶œ ì¤‘: {frame_num}/{num_samples_q}", end='\r')
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = self.cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame))
            else:
                # Padding with black frame if read fails
                frames.append(Image.new('RGB', (224, 224), (0, 0, 0)))
        
        if window_idx is not None:
            print()  # ì¤„ë°”ê¿ˆ
        return frames

    def get_timestamp_str(self, seconds):
        return str(datetime.timedelta(seconds=int(seconds)))

# ==========================================
# 3. Adaptive Search Engine (Core Logic)
# ==========================================
class AdaptiveSearchEngine:
    def __init__(self, model_manager: ModelManager, video_processor: VideoProcessor):
        self.mm = model_manager
        self.vp = video_processor
        # íƒ€ì´ë° ì •ë³´ ì €ì¥
        self.timing_info = {
            "api_call_time": 0.0,
            "clip_inference_time": 0.0,
            "blip_inference_time": 0.0,
            "frame_extraction_time": 0.0,
            "total_search_time": 0.0
        }

    def _call_gemini_with_retry(self, prompt: str, max_retries: int = 3, timeout: int = 20) -> str:
        """
        Exponential Backoffê³¼ Jitterë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§ (Timeout ì ìš©)
        
        Args:
            prompt: Gemini APIì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            timeout: ê° ìš”ì²­ì˜ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            API ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        import signal
        from contextlib import contextmanager
        
        @contextmanager
        def time_limit(seconds):
            """íƒ€ì„ì•„ì›ƒ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
            def signal_handler(signum, frame):
                raise TimeoutError(f"API í˜¸ì¶œì´ {seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
            
            # macOS/Linuxì—ì„œë§Œ signal ì‚¬ìš© ê°€ëŠ¥
            if hasattr(signal, 'SIGALRM'):
                signal.signal(signal.SIGALRM, signal_handler)
                signal.alarm(seconds)
                try:
                    yield
                finally:
                    signal.alarm(0)
            else:
                # Windowsì—ì„œëŠ” ë‹¨ìˆœ timeout
                yield
        
        # ì‹œë„í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
        models = [
            'models/gemini-2.0-flash-lite',
            'models/gemini-2.0-flash',
            'models/gemini-2.5-flash-lite'
        ]
        
        for model_idx, model in enumerate(models):
            for attempt in range(max_retries):
                try:
                    # Jitter ì¶”ê°€: 0.1~0.5ì´ˆ ëœë¤ ì§€ì—° (ë™ì‹œ ìš”ì²­ ì¶©ëŒ ë°©ì§€)
                    if attempt > 0:
                        jitter = random.uniform(0.1, 0.5)
                        time.sleep(jitter)
                    
                    if attempt == 0 and model_idx == 0:
                        print(f"  [API] {model} í˜¸ì¶œ ì¤‘... (Timeout: {timeout}ì´ˆ)")
                    else:
                        print(f"  [API] ì¬ì‹œë„ {attempt + 1}/{max_retries} (ëª¨ë¸: {model}, Timeout: {timeout}ì´ˆ)...")
                    
                    # API í˜¸ì¶œ (Timeout ì ìš©)
                    start_time = time.time()
                    try:
                        if hasattr(signal, 'SIGALRM'):
                            with time_limit(timeout):
                                response = client.models.generate_content(
                                    model=model,
                                    contents=prompt
                                )
                        else:
                            # Windows: ë‹¨ìˆœ í˜¸ì¶œ
                            response = client.models.generate_content(
                                model=model,
                                contents=prompt
                            )
                            elapsed = time.time() - start_time
                            if elapsed > timeout:
                                raise TimeoutError(f"API í˜¸ì¶œì´ {timeout}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                    except TimeoutError as e:
                        print(f"  [API] â±ï¸ Timeout ({timeout}ì´ˆ ì´ˆê³¼). ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...")
                        break  # ë‹¤ìŒ ëª¨ë¸ë¡œ
                    
                    elapsed = time.time() - start_time
                    print(f"  [API] âœ“ ì„±ê³µ! (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
                    return response.text.strip()
                    
                except Exception as e:
                    error_str = str(e)
                    
                    # 429 Rate Limit ì—ëŸ¬ ì²˜ë¦¬
                    if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str:
                        print(f"  [API] âš  Rate Limit ë„ë‹¬. ë‹¤ìŒ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì „í™˜...")
                        break  # ëŒ€ê¸°í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ëª¨ë¸ë¡œ
                    
                    # ê¸°íƒ€ ì—ëŸ¬
                    else:
                        print(f"  [API] âœ— ì—ëŸ¬ ë°œìƒ: {error_str}")
                        if attempt < max_retries - 1:
                            continue
                        else:
                            if model_idx < len(models) - 1:
                                print(f"  [API] ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜...")
                                break
                            else:
                                raise Exception(f"ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨: {error_str}")
        
        raise Exception("Gemini API í˜¸ì¶œ ì‹¤íŒ¨: ëª¨ë“  ì¬ì‹œë„ ì†Œì§„")

    def split_query(self, text_query: str) -> tuple[list[str], str]:
        """
        Gemini APIë¥¼ í™œìš©í•˜ì—¬ í•œêµ­ì–´ ì¿¼ë¦¬ë¥¼ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ë™ì‘ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, í•œêµ­ì–´ ì ‘ì†ì‚¬ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ì„ ì‹œë„í•©ë‹ˆë‹¤.
        
        Returns:
            tuple: (ë¶„í• ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ë¶„í•  ì´ìœ  ì„¤ëª…)
        """
        api_start_time = time.time()
        print(f"Thinking with Gemini (Korean Mode)... Query: '{text_query}'")
        
        # ---------------------------------------------------------
        # 1. Gemini API í˜¸ì¶œ (Primary Strategy)
        # ---------------------------------------------------------
        prompt = f"""
        ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ìœ„í•œ ì–¸ì–´ ë¶„ì„ê¸°ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´(Query)ê°€ ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì—¬ëŸ¬ ë™ì‘(Sequence)ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´, ì´ë¥¼ ë¶„í• í•˜ì„¸ìš”.
        ê·¸ í›„, ê·¸ê²ƒë“¤ì„ CLIP ëª¨ë¸ì´ ê°€ì¥ ì˜ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì •êµí•œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ë²ˆì—­í•œ í›„ JSON ê°ì²´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
        
        [ê·œì¹™]
        1. ë¬¸ë§¥ìƒ ì‹œê°„ì˜ íë¦„(ì˜ˆ: ~í•˜ê³  ë‚˜ì„œ, ~í•œ ë’¤ì—, ~í•˜ë‹¤ê°€)ì´ ìˆìœ¼ë©´ ë™ì‘ ë‹¨ìœ„ë¡œ ìª¼ê°œì„¸ìš”.
        2. ë‹¨ìˆœí•œ ë¬˜ì‚¬ë‚˜ ë‹¨ì¼ ë™ì‘ì´ë©´ ìš”ì†Œê°€ 1ê°œì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
        3. 1íšŒ ì´ìƒ ìª¼ê°œì§€ ë§ˆì„¸ìš”. (ìš”ì†ŒëŠ” 2ê°œ ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.)
        4. ë°˜í™˜ê°’ì€ ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ê°ì²´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (Markdown ì œì™¸)
        5. ë¶„í• ëœ ë¬¸ì¥ì€ ê²€ìƒ‰ì´ ì˜ ë˜ë„ë¡ ê¸°ë³¸í˜•(ì˜ˆ: 'ë‹¬ë¦¬ê³ ' -> 'ë‹¬ë¦¬ëŠ” ì‚¬ëŒ')ì´ë‚˜ ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.
        6. ë¶„í•  ì—¬ë¶€ì™€ ê·¸ ì´ìœ ë¥¼ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.

        [ë°˜í™˜ í˜•ì‹]
        {{
        "results": [
            {{"ko": "í•œêµ­ì–´ ë™ì‘ 1", "en": "English description 1"}},
            {{"ko": "í•œêµ­ì–´ ë™ì‘ 2", "en": "English description 2"}}
        ],
        "reason": "ë¶„í•  ë° ë²ˆì—­ ì´ìœ "
        }}

        [ì˜ˆì‹œ 1]
        Query: "ê³µì„ ë˜ì§€ê³  ë‚˜ì„œ ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ"
        Output: {{
            "results": [
                {{"ko": "ê³µì„ ë˜ì§€ëŠ” ì‚¬ëŒ", "en": "Person throwing a ball"}},
                {{"ko": "ë„˜ì–´ì§€ëŠ” ì‚¬ëŒ", "en": "Person falling down"}}
            ],
            "reason": "ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'ë˜ì§€ê³  ë‚˜ì„œ'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ê°œì˜ ì—°ì† ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
        }}

        [ì˜ˆì‹œ 2]
        Query: "ì›ƒê³  ìˆëŠ” ì•„ê¸°"
        Output: {{
            "results": [
                {{"ko": "ì›ƒê³  ìˆëŠ” ì•„ê¸°", "en": "Baby laughing"}}
            ],
            "reason": "ë‹¨ì¼ ë™ì‘/ìƒíƒœë¥¼ ë¬˜ì‚¬í•˜ëŠ” ì¿¼ë¦¬ë¡œ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }}

        [ì˜ˆì‹œ 3]
        Query: "ìš”ë¦¬ë¥¼ í•˜ë‹¤ê°€ ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì"
        Output: {{
            "results": [
                {{"ko": "ìš”ë¦¬ë¥¼ í•˜ëŠ” ë‚¨ì", "en": "Man cooking"}},
                {{"ko": "ë¶ˆì´ ë‚˜ì„œ ë‹¹í™©í•˜ëŠ” ë‚¨ì", "en": "Man panicking after fire"}}
            ],
            "reason": "'í•˜ë‹¤ê°€'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ìˆœì„œìƒ ì„ í–‰ ë™ì‘ê³¼ í›„í–‰ ë™ì‘ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
        }}

        [ì‹¤ì œ ì…ë ¥]
        Query: "{text_query}"
        """

        try:
            # API í˜¸ì¶œ (Retry ë¡œì§ í¬í•¨)
            result_text = self._call_gemini_with_retry(prompt)
            
            if result_text.startswith("```"):
                result_text = re.sub(r"```(json)?", "", result_text).strip()
                result_text = re.sub(r"```", "", result_text).strip()
            
            # JSON íŒŒì‹±
            result = json.loads(result_text)
            
            if isinstance(result, dict) and "results" in result:
                actions = [x["en"] for x in result["results"]]
                reason = result.get("reason", "ë¶„í•  ì™„ë£Œ")
                
                api_time = time.time() - api_start_time
                self.timing_info["api_call_time"] = api_time
                print(f" -> Gemini Split Result (EN): {actions} \n Reason: {reason}")
                print(f" -> API í˜¸ì¶œ ì‹œê°„: {api_time:.2f}ì´ˆ")
                return actions, f"[Gemini API] {reason}"
            
        except Exception as e:
            print(f"Gemini API Error: {e}. Switching to Fallback.")

        # ---------------------------------------------------------
        # 2. ê·œì¹™ ê¸°ë°˜ Fallback (í•œêµ­ì–´ ì ‘ì†ì‚¬ ì²˜ë¦¬)
        # ---------------------------------------------------------
        # APIê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‘ë‹µì´ ì´ìƒí•  ê²½ìš° ì‘ë™í•˜ëŠ” ë¹„ìƒ ë¡œì§ì…ë‹ˆë‹¤.
        # í•œêµ­ì–´ì—ì„œ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”í•œ í‘œí˜„ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ìë¦…ë‹ˆë‹¤.
        
        api_time = time.time() - api_start_time
        self.timing_info["api_call_time"] = api_time
        
        delimiters = [
            " ê·¸ë¦¬ê³  ", " ë‹¤ìŒì— ", " ê·¸ í›„ ", " ê·¸ ë’¤ì— ", " ë‚˜ì„œ ", 
            " í•˜ë‹¤ê°€ ", "ë‹¤ê°€ "
        ]
        
        # ê°€ì¥ ë¨¼ì € ë°œê²¬ë˜ëŠ” êµ¬ë¶„ìë¡œ 1íšŒë§Œ ë¶„í•  ì‹œë„ (ë³µì¡ì„± ë°©ì§€)
        for delim in delimiters:
            if delim in text_query:
                parts = text_query.split(delim)
                # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ê³µë°± ì •ë¦¬
                clean_parts = [p.strip() for p in parts if p.strip()]
                if len(clean_parts) > 1:
                    print(f" -> Rule-based Split Result: {clean_parts}")
                    print(f" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ")
                    return clean_parts, f"[Rule-based] '{delim.strip()}' êµ¬ë¶„ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤."

        # ë¶„í•  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        print("Final split result: ", [text_query])
        print(f" -> Fallback ì²˜ë¦¬ ì‹œê°„: {api_time:.2f}ì´ˆ")
        return [text_query], "[Rule-based] ì‹œê°„ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ì´ ì—†ì–´ ë¶„í• í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    def calculate_sequential_score(self, frames, sub_queries):
        """
        [ë³€ê³¡ì  íƒì§€ ë¡œì§]
        ì¿¼ë¦¬ê°€ A -> Bë¡œ ë‚˜ë‰˜ì—ˆì„ ë•Œ, í”„ë ˆì„ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ìµœì ì˜ ë¶„í•  ì§€ì ì„ ì°¾ì•„
        (Aìœ ì‚¬ë„ + Bìœ ì‚¬ë„)ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Returns: (max_score, scores_matrix, best_split_index)
        """
        # (q_frames, 2_sub_queries) matrix
        scores_matrix = self.mm.get_clip_scores(frames, sub_queries) 
        q_len = len(frames)
        max_score = -1.0
        best_split = -1
        
        # Linear Scan to find Change Point
        # ìµœì†Œ 20% ì§€ì ë¶€í„° 80% ì§€ì  ì‚¬ì´ì—ì„œ ë¶„í•  ì‹œë„
        start_idx = int(q_len * 0.2)
        end_idx = int(q_len * 0.8)

        if len(sub_queries) == 2:
            score_A = scores_matrix[:, 0] # Similarity curve for Query A
            score_B = scores_matrix[:, 1] # Similarity curve for Query B
            
            for t in range(start_idx, end_idx):
                # t ì‹œì ê¹Œì§€ëŠ” A, t ì´í›„ëŠ” B
                avg_A = np.mean(score_A[:t])
                avg_B = np.mean(score_B[t:])
                combined_score = (avg_A + avg_B) / 2
                
                if combined_score > max_score:
                    max_score = combined_score
                    best_split = t
        else:
            max_score = np.mean(np.max(scores_matrix, axis=1))

        return float(max_score), scores_matrix, best_split

    def normalize_score(self, raw_score: float) -> float:
        """
        CLIP ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ 0-100 ì ìˆ˜ë¡œ ë³€í™˜
        - 0.2 ì´í•˜: 0ì 
        - 0.45 ì´ìƒ: 100ì 
        """
        lower_bound = 0.20
        upper_bound = 0.45
        
        # ì •ê·œí™” ê³„ì‚°
        normalized = (raw_score - lower_bound) / (upper_bound - lower_bound) * 100
        
        # 0~100 ì‚¬ì´ë¡œ í´ë¦¬í•‘
        return float(np.clip(normalized, 0, 100))

    def search(self, original_query, sub_queries, p_sec, q_frames, k_top, weight_clip = 0.7, weight_semantic = 0.3, enable_visualization=True, save_path="results"):
        """
        Adaptive Search Engine ì‹¤í–‰ ë©”ì¸ ë¡œì§
        - 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)
        - 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)
        - 3. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ ë° ì •ë ¬
        
        Args:
            enable_visualization: ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™” ì—¬ë¶€
            save_path: ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
        """
        search_start_time = time.time()
        
        # íƒ€ì´ë° ì´ˆê¸°í™”
        total_frame_extraction_time = 0.0
        total_clip_inference_time = 0.0
        total_blip_inference_time = 0.0
        
        is_sequential = len(sub_queries) > 1
        
        all_windows = []
        step_size = p_sec  # ìœˆë„ìš°ê°€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ìˆ˜ì •
        current_time = 0.0
        
        # ì „ì²´ ìœˆë„ìš° ê°œìˆ˜ ê³„ì‚° (ì •í™•í•œ ê³„ì‚°)
        total_windows = int(np.ceil(self.vp.duration / step_size))
        
        # ì‹¤ì‹œê°„ ì‹œê°í™” ì´ˆê¸°í™”
        visualizer = None
        if enable_visualization:
            try:
                visualizer = RealTimeVisualizer(self.vp.duration, k_top, save_path)
                print(f"\n{'='*60}")
                print(f"[ğŸ“Š ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”] ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ì— í‘œì‹œí•©ë‹ˆë‹¤!")
                print(f"{'='*60}\n")
            except Exception as e:
                print(f"ì‹œê°í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. ì‹œê°í™” ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                visualizer = None
        
        print(f"\n{'='*60}")
        print(f"[ê²€ìƒ‰ ì‹œì‘] ì´ {total_windows}ê°œ ìœˆë„ìš° ì²˜ë¦¬ ì˜ˆì • (ìœˆë„ìš° í¬ê¸°: {p_sec}ì´ˆ, í”„ë ˆì„ ìƒ˜í”Œ: {q_frames}ê°œ)")
        print(f"{'='*60}\n")
        
        # 1. CLIP ê¸°ë°˜ 1ì°¨ ê²€ìƒ‰ (Coarse-grained Search)
        window_idx = 0
        current_top_window = None
        
        while current_time < self.vp.duration:
            window_idx += 1
            end_time = min(current_time + p_sec, self.vp.duration)
            
            print(f"[Window {window_idx}/{total_windows}] ì²˜ë¦¬ ì¤‘: {self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}")
            
            # í”„ë ˆì„ ì¶”ì¶œ ì‹œê°„ ì¸¡ì •
            frame_start = time.time()
            frames = self.vp.extract_window_frames(current_time, end_time, q_frames, window_idx, total_windows)
            frame_time = time.time() - frame_start
            total_frame_extraction_time += frame_time
            
            # CLIP ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            clip_start = time.time()
            if is_sequential:
                raw_score, scores_matrix, best_split = self.calculate_sequential_score(frames, sub_queries)
                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥ (ì‹œí€€ì…œì˜ ê²½ìš° ë‘ ì¿¼ë¦¬ì— ëŒ€í•œ ì ìˆ˜)
                frame_scores = {
                    f"query_{i}": scores_matrix[:, i].tolist() 
                    for i in range(len(sub_queries))
                }
                frame_scores["best_split_index"] = int(best_split) if best_split != -1 else None
            else:
                raw_scores_matrix = self.mm.get_clip_scores(frames, sub_queries)
                raw_score = float(np.mean(raw_scores_matrix))
                # ê° í”„ë ˆì„ë³„ ì ìˆ˜ ì €ì¥
                frame_scores = {
                    f"query_{i}": raw_scores_matrix[:, i].tolist() 
                    for i in range(len(sub_queries))
                }
            clip_time = time.time() - clip_start
            total_clip_inference_time += clip_time
            
            clip_score_norm = self.normalize_score(raw_score)
            print(f"  -> ì •ê·œí™” CLIP ì ìˆ˜: {clip_score_norm:.2f} (í”„ë ˆì„ ì¶”ì¶œ: {frame_time:.2f}ì´ˆ, CLIP ì¶”ë¡ : {clip_time:.2f}ì´ˆ)")
            
            window_data = {
                "start": current_time,
                "end": end_time,
                "timestamp": f"{self.vp.get_timestamp_str(current_time)} - {self.vp.get_timestamp_str(end_time)}",
                "raw_score": raw_score,           # ì°¸ê³ ìš© ì›ë³¸ ì ìˆ˜ 
                "clip_score_norm": clip_score_norm,    # ì •ê·œí™”ëœ ì ìˆ˜ (JSON ì €ì¥ìš©)
                "frame_scores": frame_scores,  # í”„ë ˆì„ë³„ ì ìˆ˜ ì¶”ê°€
                "mid_frame": frames[len(frames)//2] # ë³´ì •ì„ ìœ„í•´ ì¤‘ê°„ í”„ë ˆì„ ì €ì¥
            }
            all_windows.append(window_data)
            
            # í˜„ì¬ê¹Œì§€ ìµœê³  ì ìˆ˜ ìœˆë„ìš° ì¶”ì 
            if current_top_window is None or clip_score_norm > current_top_window['clip_score_norm']:
                current_top_window = window_data
                print(f"  â­ ìƒˆë¡œìš´ Top ìœˆë„ìš° ë°œê²¬! ({current_top_window['timestamp']})\n")
            else:
                print(f"  [í˜„ì¬ Top] {current_top_window['timestamp']} (ì ìˆ˜: {current_top_window['clip_score_norm']:.4f})\n")
            
            # ì‹¤ì‹œê°„ ì‹œê°í™” ì—…ë°ì´íŠ¸
            if visualizer:
                try:
                    visualizer.update({
                        'start': current_time,
                        'end': end_time,
                        'clip_score_norm': clip_score_norm
                    })
                except Exception as e:
                    print(f"  [ì‹œê°í™” ê²½ê³ ] ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
            current_time += step_size

        # CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ Kê°œ ì„ ë³„
        print(f"\n{'='*60}")
        print(f"[1ì°¨ ê²€ìƒ‰ ì™„ë£Œ] CLIP ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ {k_top}ê°œ í›„ë³´ ì„ ë³„")
        print(f"{'='*60}")
        
        all_windows.sort(key=lambda x: x["clip_score_norm"], reverse=True)
        top_k_candidates = all_windows[:k_top]
        
        for idx, item in enumerate(top_k_candidates, 1):
            print(f"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}")

        # 2. BLIP-2 ê¸°ë°˜ 2ì°¨ ë³´ì • (Fine-grained Refinement)
        if self.mm.use_blip:
            print(f"\n{'='*60}")
            print(f"[2ì°¨ ë³´ì • ì‹œì‘] BLIP-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ {k_top}ê°œ í›„ë³´ ë³´ì • ì¤‘...")
            print(f"{'='*60}\n")
            
            for idx, item in enumerate(top_k_candidates, 1):
                print(f"[í›„ë³´ {idx}/{k_top}] {item['timestamp']}")
                
                # A. BLIP-2ë¡œ í”„ë ˆì„ ì„¤ëª…(Caption) ìƒì„± - ì‹œê°„ ì¸¡ì •
                blip_start = time.time()
                generated_caption = self.mm.generate_caption(item['mid_frame'])
                blip_time = time.time() - blip_start
                total_blip_inference_time += blip_time
                
                item['blip_caption'] = generated_caption
                
                # B. ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ìƒì„±ëœ ìº¡ì…˜ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (Text-to-Text)
                semantic_start = time.time()
                semantic_sim = self.mm.compute_text_similarity(original_query, generated_caption)
                semantic_time = time.time() - semantic_start
                total_blip_inference_time += semantic_time
                
                item['semantic_consistency'] = semantic_sim

                # C. ìµœì¢… ì ìˆ˜ ì‚°ì¶œ (ì•™ìƒë¸”)
                item['final_score'] = (item['clip_score_norm'] * weight_clip) + (semantic_sim * weight_semantic)
                
                print(f"  -> ìƒì„±ëœ ìº¡ì…˜: {generated_caption}")
                print(f"  -> ì˜ë¯¸ ìœ ì‚¬ë„: {semantic_sim:.4f}")
                print(f"  -> ìµœì¢… ì ìˆ˜: {item['final_score']:.4f}")
                print(f"  -> BLIP-2 ì²˜ë¦¬ ì‹œê°„: {blip_time + semantic_time:.2f}ì´ˆ\n")
            
            # ë³´ì •ëœ ìµœì¢… ì ìˆ˜ë¡œ ë‹¤ì‹œ ì •ë ¬
            top_k_candidates.sort(key=lambda x: x.get('final_score', x['clip_score_norm']), reverse=True)
            
            print(f"{'='*60}")
            print(f"[ìµœì¢… ìˆœìœ„]")
            print(f"{'='*60}")
            for idx, item in enumerate(top_k_candidates, 1):
                print(f"{idx}. {item['timestamp']} - ìµœì¢… ì ìˆ˜: {item.get('final_score', item['clip_score_norm']):.4f}")
        else:
            # BLIP-2 ì—†ì„ ë•Œë„ ìµœì¢… ìˆœìœ„ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"[ìµœì¢… ìˆœìœ„]")
            print(f"{'='*60}")
            for idx, item in enumerate(top_k_candidates, 1):
                print(f"{idx}. {item['timestamp']} - ì ìˆ˜: {item['clip_score_norm']:.4f}")
        
        print()

        # ì‹¤ì‹œê°„ ì‹œê°í™” ìµœì¢… ì—…ë°ì´íŠ¸ (BLIP-2 ë³´ì • ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ì—¬ê¸°ì„œ í˜¸ì¶œ)
        if visualizer:
            try:
                visualizer.finalize(top_k_candidates)
                print(f"\n{'='*60}")
                print(f"[ğŸ“Š ì‹œê°í™” ì™„ë£Œ] ìµœì¢… Top-{k_top} ê²°ê³¼ê°€ ë¹¨ê°„ìƒ‰ ë³„(â˜…)ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"{'='*60}\n")
            except Exception as e:
                print(f"[ì‹œê°í™” ì˜¤ë¥˜] ìµœì¢… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # ê²°ê³¼ ì €ì¥ ì „ ì´ë¯¸ì§€ ê°ì²´ ì‚­ì œ (ë©”ëª¨ë¦¬ í™•ë³´)
        for item in top_k_candidates:
            if 'mid_frame' in item: del item['mid_frame']
        
        # all_windowsì—ì„œë„ ì´ë¯¸ì§€ ê°ì²´ ì‚­ì œ
        for item in all_windows:
            if 'mid_frame' in item: del item['mid_frame']
        
        # íƒ€ì´ë° ì •ë³´ ì €ì¥
        total_search_time = time.time() - search_start_time
        self.timing_info["frame_extraction_time"] = total_frame_extraction_time
        self.timing_info["clip_inference_time"] = total_clip_inference_time
        self.timing_info["blip_inference_time"] = total_blip_inference_time
        self.timing_info["total_search_time"] = total_search_time
        
        # visualizer ê°ì²´ë¥¼ ë°˜í™˜ (mainì—ì„œ ì €ì¥)
        return top_k_candidates, all_windows, visualizer

# ==========================================
# 4. Real-time Visualization
# ==========================================
class RealTimeVisualizer:
    def __init__(self, total_duration, k_top, save_path="results"):
        """
        ì‹¤ì‹œê°„ ì‹œê°í™”ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
        
        Args:
            total_duration: ë¹„ë””ì˜¤ ì´ ê¸¸ì´ (ì´ˆ)
            k_top: Top-K ê°œìˆ˜
            save_path: ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
        """
        self.total_duration = total_duration
        self.k_top = k_top
        self.save_path = save_path
        self.window_data = []
        self.current_top_k = []
        self.is_complete = False
        self.save_filename = None
        
        # ê·¸ë˜í”„ ì„¤ì •
        plt.ion()  # Interactive mode
        self.fig, self.ax = plt.subplots(figsize=(14, 6))
        self.fig.suptitle('Real-time Video Search Similarity Scores', fontsize=14, fontweight='bold')
        
    def update(self, window_info):
        """
        ìƒˆë¡œìš´ ìœˆë„ìš° ì •ë³´ë¡œ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        
        Args:
            window_info: {'start': float, 'end': float, 'clip_score_norm': float, 'is_top_k': bool}
        """
        self.window_data.append(window_info)
        
        # í˜„ì¬ê¹Œì§€ì˜ Top-K ê³„ì‚°
        sorted_windows = sorted(self.window_data, key=lambda x: x['clip_score_norm'], reverse=True)
        self.current_top_k = sorted_windows[:self.k_top]
        
        self._draw()
        
    def finalize(self, final_top_k):
        """
        ê²€ìƒ‰ ì™„ë£Œ í›„ ìµœì¢… Top-K í‘œì‹œ
        
        Args:
            final_top_k: ìµœì¢… Top-K ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸
        """
        self.is_complete = True
        self.final_top_k = final_top_k
        self._draw()
        
    def _draw(self):
        """ê·¸ë˜í”„ ê·¸ë¦¬ê¸°"""
        self.ax.clear()
        
        if not self.window_data:
            return
        
        # ì‹œê°„ì¶•ê³¼ ì ìˆ˜ ë°ì´í„° ì¤€ë¹„
        times = [(w['start'] + w['end']) / 2 for w in self.window_data]
        scores = [w['clip_score_norm'] for w in self.window_data]
        
        # 1. ê¸°ë³¸ ì ìˆ˜ ì„  ê·¸ë˜í”„ (íšŒìƒ‰)
        self.ax.plot(times, scores, color='#CCCCCC', linewidth=1, alpha=0.6, zorder=1)
        
        # 2. ëª¨ë“  ìœˆë„ìš° ì  (ì‘ì€ íŒŒë€ìƒ‰)
        self.ax.scatter(times, scores, color='#4A90E2', s=30, alpha=0.5, zorder=2)
        
        # 3. í˜„ì¬ Top-K í›„ë³´ (ë…¸ë€ìƒ‰ í° ì )
        if not self.is_complete:
            top_k_times = [(w['start'] + w['end']) / 2 for w in self.current_top_k]
            top_k_scores = [w['clip_score_norm'] for w in self.current_top_k]
            self.ax.scatter(top_k_times, top_k_scores, color='#FFD700', s=200, 
                          edgecolors='#FFA500', linewidths=2, zorder=4, 
                          label=f'Current Top-{self.k_top}', marker='o', alpha=0.9)
            
            # ë°˜ì§ì´ëŠ” íš¨ê³¼ë¥¼ ìœ„í•œ ì™¸ê³½ì„ 
            for t, s in zip(top_k_times, top_k_scores):
                circle = Circle((t, s), radius=0.3, color='#FFD700', alpha=0.3, zorder=3)
                self.ax.add_patch(circle)
        
        # 4. ìµœì¢… Top-K (ë¹¨ê°„ìƒ‰ í° ì )
        if self.is_complete:
            final_times = [(w['start'] + w['end']) / 2 for w in self.final_top_k]
            final_scores = [w['clip_score_norm'] for w in self.final_top_k]
            self.ax.scatter(final_times, final_scores, color='#E74C3C', s=250, 
                          edgecolors='#C0392B', linewidths=3, zorder=5, 
                          label=f'Final Top-{self.k_top}', marker='*', alpha=1.0)
            
            # ìˆœìœ„ í‘œì‹œ
            for idx, (t, s, w) in enumerate(zip(final_times, final_scores, self.final_top_k), 1):
                self.ax.annotate(f'#{idx}', xy=(t, s), xytext=(5, 5), 
                               textcoords='offset points', fontsize=10, 
                               fontweight='bold', color='#E74C3C',
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='#E74C3C', alpha=0.8))
        
        # ê·¸ë˜í”„ ì„¤ì •
        self.ax.set_xlabel('Video Time (seconds)', fontsize=11, fontweight='bold')
        self.ax.set_ylabel('Normalized Similarity Score', fontsize=11, fontweight='bold')
        self.ax.set_xlim(0, self.total_duration)
        self.ax.set_ylim(0, 105)
        self.ax.grid(True, alpha=0.3, linestyle='--')
        self.ax.legend(loc='upper right', fontsize=9)
        
        # ì§„í–‰ë¥  í‘œì‹œ
        if self.window_data:
            progress = (self.window_data[-1]['end'] / self.total_duration) * 100
            status = "COMPLETE âœ“" if self.is_complete else f"Processing... {progress:.1f}%"
            self.ax.text(0.02, 0.98, status, transform=self.ax.transAxes, 
                       fontsize=11, fontweight='bold', verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.pause(0.01)
        
    def save_and_close(self, filename_base):
        """ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ê³  ì°½ ë‹«ê¸°"""
        plt.ioff()
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_filename = f"viz_{filename_base}.png"
        save_path = os.path.join(self.save_path, self.save_filename)
        
        # ì´ë¯¸ì§€ë¡œ ì €ì¥
        self.fig.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  [ì‹œê°í™”] ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {self.save_filename}")
        
        # ì°½ ë‹«ê¸°
        plt.close(self.fig)
        
        return self.save_filename

# ==========================================
# 5. Main Execution
# ==========================================
def main():
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
    program_start_time = time.time()
    
    # --- Configurations ---
    VIDEO_PATH = "sample_video.mp4" # ì¤€ë¹„ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    SAVE_PATH = "results"
    if not os.path.exists(SAVE_PATH):
        os.makedirs(SAVE_PATH)
    else:
        print(f"Save path '{SAVE_PATH}' already exists. Results will be saved here.")
    QUERY = "ë°”ë‹¥ì— ë–¨ì–´ì§„ ì‹ ìš©ì¹´ë“œ"
    # "ë°”ë‹¥ì— ë–¨ì–´ì§€ëŠ” ì¹´ë“œë¥¼ ë³´ê³  ë‚œê°í•œ í‘œì •ì„ ì§“ëŠ” ë‚¨ì" # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    
    # Experiments Parameters
    p_list = [2.0, 4.0]      # ìœˆë„ìš° í¬ê¸° (ì´ˆ)
    q_list = [12, 24, 48]         # ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜
    k_list = [3, 5]          # Top-K ê°œìˆ˜
    USE_BLIP = False         # BLIP-2 ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì£¼ì˜)
    WEIGHT_CLIP = 0.7
    WEIGHT_SEMANTIC = 0.3
    USE_LOOP = False         # ë°˜ë³µ ì‹¤í–‰ ì—¬ë¶€

    # Initialize
    if not os.path.exists(VIDEO_PATH):
        print(f"Error: Video file '{VIDEO_PATH}' not found. Please place a dummy video.")
        return

    # ì´ˆê¸°í™” ì‹œê°„ ì¸¡ì •
    init_start_time = time.time()
    model_manager = ModelManager(use_blip=USE_BLIP)
    video_processor = VideoProcessor(VIDEO_PATH)
    engine = AdaptiveSearchEngine(model_manager, video_processor)
    total_init_time = time.time() - init_start_time
    
    print(f"\n[ì¿¼ë¦¬ ë¶„ì„] '{QUERY}'")
    sub_queries, split_reason = engine.split_query(QUERY)
    print(f"[ë¶„í• ëœ ì¿¼ë¦¬] {sub_queries}\n")
    
    # Experiment Loop
    # ë°˜ë³µ ì‹¤í–‰ í•  ë•Œ
    if USE_LOOP:
        for p in p_list:
            for q in q_list:
                for k in k_list:
                    print(f"\n--- Running Experiment: p={p}, q={q}, k={k} ---")
                    
                    # Perform Search (ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”)
                    results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p, q, k, WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)
                    
                    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
                    total_elapsed_time = time.time() - program_start_time
                    
                    # Construct Filename
                    model_name = "CB" if USE_BLIP else "Clip"
                    timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    # ìœ ì˜ë¯¸í•œ ê²°ê³¼ ë‚˜ì™”ìœ¼ë©´ _test.json ëŒ€ì‹  .json í™•ì¥ì ì‚¬ìš©
                    filename = f"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json"
                    filename_base = f"{model_name}_{p}, {q}, {k}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test"
                    
                    # ì‹œê°í™” ì €ì¥
                    if visualizer:
                        try:
                            viz_filename = visualizer.save_and_close(filename_base)
                        except Exception as e:
                            print(f"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}")
                            viz_filename = None
                    else:
                        viz_filename = None
                    
                    # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
                    timing_data = {
                        "total_time": round(total_elapsed_time, 2),
                        "init_time": round(total_init_time, 2),
                        "model_manager_init_time": round(model_manager.init_time, 2),
                        "clip_load_time": round(model_manager.clip_load_time, 2),
                        "blip_load_time": round(model_manager.blip_load_time, 2),
                        "video_processor_init_time": round(video_processor.init_time, 2),
                        "api_call_time": round(engine.timing_info["api_call_time"], 2),
                        "frame_extraction_time": round(engine.timing_info["frame_extraction_time"], 2),
                        "clip_inference_time": round(engine.timing_info["clip_inference_time"], 2),
                        "blip_inference_time": round(engine.timing_info["blip_inference_time"], 2),
                        "total_search_time": round(engine.timing_info["total_search_time"], 2)
                    }
                    
                    # Output Data Structure
                    output_data = {
                        "meta": {
                            "video_path": VIDEO_PATH,
                            "query": QUERY,
                            "sub_queries": sub_queries,
                            "split_reason": split_reason,
                            "parameters": {"p": p, "q": q, "k": k, "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                            "model": model_name,
                            "timestamp": timestamp_str
                        },
                        "time_used": timing_data,
                        "results": results
                    }
                    
                    # Save to JSON
                    with open(os.path.join(SAVE_PATH, filename), "w", encoding='utf-8') as f:
                        json.dump(output_data, f, indent=4, ensure_ascii=False)
                    
                    # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥
                    whole_score_filename = f"whole_score_{filename}"
                    whole_score_data = {
                        "meta": {
                            "video_path": VIDEO_PATH,
                            "query": QUERY,
                            "sub_queries": sub_queries,
                            "parameters": {"p": p, "q": q, "k": k, "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                            "total_windows": len(all_windows_data),
                            "timestamp": timestamp_str
                        },
                        "all_windows": all_windows_data
                    }
                    with open(os.path.join(SAVE_PATH, whole_score_filename), "w", encoding='utf-8') as f:
                        json.dump(whole_score_data, f, indent=4, ensure_ascii=False)
                    
                    print(f"\n[ì €ì¥ ì™„ë£Œ] {filename}")
                    print(f"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}")
                    print(f"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨")
                    if viz_filename:
                        print(f"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}")
                    print(f"[ì´ ì‹¤í–‰ ì‹œê°„] {total_elapsed_time:.2f}ì´ˆ\n")
                    
    # ë°˜ë³µ ì‹¤í–‰ ì•„ë‹ ë•Œ
    else:
        results, all_windows_data, visualizer = engine.search(QUERY, sub_queries, p_list[0], q_list[0], k_list[0], WEIGHT_CLIP, WEIGHT_SEMANTIC, enable_visualization=True, save_path=SAVE_PATH)
        
        # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        total_elapsed_time = time.time() - program_start_time
        
        model_name = "CB" if USE_BLIP else "Clip"
        timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test.json"
        filename_base = f"{model_name}_{p_list[0]}, {q_list[0]}, {k_list[0]}, {USE_BLIP}, {WEIGHT_CLIP if USE_BLIP else ''}, {WEIGHT_SEMANTIC if USE_BLIP else ''}_{timestamp_str}_test"
        
        # ì‹œê°í™” ì €ì¥
        if visualizer:
            try:
                viz_filename = visualizer.save_and_close(filename_base)
            except Exception as e:
                print(f"  [ì‹œê°í™”] ì €ì¥ ì‹¤íŒ¨: {e}")
                viz_filename = None
        else:
            viz_filename = None
        
        # íƒ€ì´ë° ì •ë³´ ìˆ˜ì§‘
        timing_data = {
            "total_time": round(total_elapsed_time, 2),
            "init_time": round(total_init_time, 2),
            "model_manager_init_time": round(model_manager.init_time, 2),
            "clip_load_time": round(model_manager.clip_load_time, 2),
            "blip_load_time": round(model_manager.blip_load_time, 2),
            "video_processor_init_time": round(video_processor.init_time, 2),
            "api_call_time": round(engine.timing_info["api_call_time"], 2),
            "frame_extraction_time": round(engine.timing_info["frame_extraction_time"], 2),
            "clip_inference_time": round(engine.timing_info["clip_inference_time"], 2),
            "blip_inference_time": round(engine.timing_info["blip_inference_time"], 2),
            "total_search_time": round(engine.timing_info["total_search_time"], 2)
        }
        
        # Output Data Structure
        output_data = {
            "meta": {
                "video_path": VIDEO_PATH,
                "query": QUERY,
                "sub_queries": sub_queries,
                "split_reason": split_reason,
                "parameters": {"p": p_list[0], "q": q_list[0], "k": k_list[0], "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                "model": model_name,
                "timestamp": timestamp_str
            },
            "time_used": timing_data,
            "results": results
        }
        
        # Save to JSON
        with open(os.path.join(SAVE_PATH, filename), "w", encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)
        
        # ëª¨ë“  ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥
        whole_score_filename = f"whole_score_{filename}"
        whole_score_data = {
            "meta": {
                "video_path": VIDEO_PATH,
                "query": QUERY,
                "sub_queries": sub_queries,
                "parameters": {"p": p_list[0], "q": q_list[0], "k": k_list[0], "USE_BLIP": USE_BLIP, "WEIGHT_CLIP": WEIGHT_CLIP, "WEIGHT_SEMANTIC": WEIGHT_SEMANTIC},
                "total_windows": len(all_windows_data),
                "timestamp": timestamp_str
            },
            "all_windows": all_windows_data
        }
        with open(os.path.join(SAVE_PATH, whole_score_filename), "w", encoding='utf-8') as f:
            json.dump(whole_score_data, f, indent=4, ensure_ascii=False)
        
        print(f"\n{'='*60}")
        print(f"[ê²€ìƒ‰ ì™„ë£Œ] ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"[ìƒì„¸ ì ìˆ˜ ì €ì¥ ì™„ë£Œ] {whole_score_filename}")
        print(f"  -> ì´ {len(all_windows_data)}ê°œ ìœˆë„ìš°ì˜ ìƒì„¸ ì ìˆ˜ ì €ì¥ë¨")
        if viz_filename:
            print(f"[ì‹œê°í™” ì €ì¥ ì™„ë£Œ] {viz_filename}")
        print(f"{'='*60}")
        print(f"\nğŸ“Š [ì „ì²´ ì‹¤í–‰ ì‹œê°„ ë¶„ì„]")
        print(f"{'='*60}")
        print(f"  â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ")
        print(f"\n  ğŸ”§ ì´ˆê¸°í™” ë‹¨ê³„:")
        print(f"     - ModelManager ì´ˆê¸°í™”: {model_manager.init_time:.2f}ì´ˆ")
        print(f"       â”œâ”€ CLIP ë¡œë“œ: {model_manager.clip_load_time:.2f}ì´ˆ")
        print(f"       â””â”€ BLIP-2 ë¡œë“œ: {model_manager.blip_load_time:.2f}ì´ˆ")
        print(f"     - VideoProcessor ì´ˆê¸°í™”: {video_processor.init_time:.2f}ì´ˆ")
        print(f"     - ì „ì²´ ì´ˆê¸°í™”: {total_init_time:.2f}ì´ˆ")
        print(f"\n  ğŸ” ê²€ìƒ‰ ë‹¨ê³„:")
        print(f"     - API í˜¸ì¶œ (ì¿¼ë¦¬ ë¶„ì„): {engine.timing_info['api_call_time']:.2f}ì´ˆ")
        print(f"     - í”„ë ˆì„ ì¶”ì¶œ: {engine.timing_info['frame_extraction_time']:.2f}ì´ˆ")
        print(f"     - CLIP ì¶”ë¡ : {engine.timing_info['clip_inference_time']:.2f}ì´ˆ")
        if USE_BLIP:
            print(f"     - BLIP-2 ì¶”ë¡ : {engine.timing_info['blip_inference_time']:.2f}ì´ˆ")
        print(f"     - ì „ì²´ ê²€ìƒ‰: {engine.timing_info['total_search_time']:.2f}ì´ˆ")
        print(f"{'='*60}\n")

if __name__ == "__main__":
    main()